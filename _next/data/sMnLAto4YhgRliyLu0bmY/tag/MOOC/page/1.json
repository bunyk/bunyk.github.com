{"pageProps":{"pageNumber":"1","posts":[{"id":"edx_writing","title":"Academic and Business Writing course notes","date":"2021-01-01","toc":true,"tags":["MOOC","notes","on writing"],"excerpt":"<p>This post will contain my notes and assignments from the edX course <a href=\"https://courses.edx.org/courses/course-v1:BerkeleyX+ColWri2.2x+1T2020/course/\">Academic and Business Writing</a>.</p>\n<p>I would rearrange notes in order that seems more logical for the writing process, not in the order information was provided in course.</p>\n<p>In general writing process could be organized in following steps:</p>\n<ol>\n<li>Reading. To be a good writer you need to be active reader.</li>\n<li>Prewriting. All the stuff you do before you get down to actual writing.</li>\n<li>Writing draft, trying to use correct vocabulary.</li>\n<li>Revision of that draft to produce next draft with fixed writing problems.</li>\n<li>Repeating step 4 again for more low-level improvements of text (editing).</li>\n</ol>\n<p>This steps and ways to get better at them will be detailed in sections below.</p>","content":"<p>This post will contain my notes and assignments from the edX course <a href=\"https://courses.edx.org/courses/course-v1:BerkeleyX+ColWri2.2x+1T2020/course/\">Academic and Business Writing</a>.</p>\n<p>I would rearrange notes in order that seems more logical for the writing process, not in the order information was provided in course.</p>\n<p>In general writing process could be organized in following steps:</p>\n<ol>\n<li>Reading. To be a good writer you need to be active reader.</li>\n<li>Prewriting. All the stuff you do before you get down to actual writing.</li>\n<li>Writing draft, trying to use correct vocabulary.</li>\n<li>Revision of that draft to produce next draft with fixed writing problems.</li>\n<li>Repeating step 4 again for more low-level improvements of text (editing).</li>\n</ol>\n<p>This steps and ways to get better at them will be detailed in sections below.</p>\n<h2>Reading</h2>\n<p><strong>Interrogate</strong> - to question closely, aggressively and formally.</p>\n<p><strong>Interrogate text</strong> - to read it critically, writing down questions. What that means? Why author came to that conclusion?...</p>\n<p>To really understand text, write down it's ideas in your own words.</p>\n<p>Make outline of text.</p>\n<p>Write summary after finishing chapter or document. What were main ideas there?</p>\n<p>Analyze. What is the evidence provided? Is logic correct?</p>\n<p>Compare and contrast with other texts. Does it agree or disagree?</p>\n<p>Reading is actually a conversation with an author.</p>\n<p><strong>Annotating</strong> is almost synonymous with taking notes, but refers to making notes directly in text - underlining/highlighting, writing on margins... You could use post-its to add annotations in book you don't want to damage.</p>\n<p>As an exercise for this chapter it was advised to write notes for the article <a href=\"https://www.nytimes.com/2011/03/06/magazine/06Riff-t.html\">What I Really Want Is Someone Rolling Around in the Text</a>. Here are <a href=\"#taking-notes\">my notes</a>.</p>\n<h2>Prewriting (informal planning)</h2>\n<p>Will improve writing quality and save your time on revisiting and rewriting.</p>\n<p>Common activities include:</p>\n<ul>\n<li>idea maps</li>\n<li>outlines</li>\n<li>freewriting</li>\n</ul>\n<p>But also following, we will look in more details:</p>\n<h3>Cubing</h3>\n<p>Look at your topic from 6 points of view (sides of cube):</p>\n<ol>\n<li><strong>Describe</strong> topic. Its shape, size, color, sound... Use all your senses.</li>\n<li><strong>Compare</strong> to related topics. How is it different or similar?</li>\n<li><strong>Associate</strong>. What does it make you think about?</li>\n<li><strong>Analyze</strong> its parts. How they connect? Have they equal or different importance?</li>\n<li><strong>Apply</strong>. What could be done with it? What's the use</li>\n<li><strong>Argue</strong> for and against the topic. How could someone disagree with it?</li>\n</ol>\n<h3>The matrix</h3>\n<p>Multi-point comparison of parts of topic with questions or points.</p>\n<p><em>This basically was just formatting facts about your topic in a table. Not sure it will work with all topics, or I did not got it correctly</em></p>\n<h3>Interviewing topic</h3>\n<p>Imagine topic is a person and ask it questions:</p>\n<ol>\n<li>Your full name? Does someone know you by different name?</li>\n<li>How does dictionary or encyclopedia define you?</li>\n<li>Where, how and when you are born?</li>\n<li>Are you still alive? How you died?</li>\n<li>To what group do you belong? Who are others in your group?</li>\n<li>Can you be divided in parts? How?</li>\n<li>Were you different in the past? Will you change in the future?</li>\n<li>What's your purpose?</li>\n<li>What is similar or different from you?</li>\n<li>What are some interesting facts or statistics about you?</li>\n<li>Who could I talk to about you? Are there any experts?</li>\n<li>Quotes about you?</li>\n<li>News stories about you?\n...</li>\n</ol>\n<h2>Drafting and revision</h2>\n<p>Creating a draft:</p>\n<ol>\n<li>Write outline</li>\n<li>Write introduction</li>\n<li>Write paragraphs</li>\n<li>Write conclusion</li>\n</ol>\n<p>After the first draft comes revision(s). You don't focus on grammar and spelling at first, first you reorganize sections, delete unnecessary, add missing, provide additional examples, add clarity to arguments, etc...</p>\n<p>Revision checklist:</p>\n<ol>\n<li>Is title interesting and descriptive?</li>\n<li>Does opening catches attention?</li>\n<li>Will your readers learn something interesting?</li>\n<li>Is it organized logically?</li>\n<li>Do you support your ideas with examples?</li>\n<li>Is there unnecessary information to remove?</li>\n<li>Are transitions from one idea to next effective?</li>\n<li>Is conclusion satisfying and leaves the reader with a food for thought?</li>\n<li>Tone and diction</li>\n<li>Vocabulary</li>\n<li>Punctuation</li>\n<li>Grammar</li>\n</ol>\n<p>Some tricks to catch errors:</p>\n<ul>\n<li>Read the text backwards</li>\n<li>Read it aloud</li>\n<li>Listen to it using text to speech software.</li>\n</ul>\n<p>Fixing vocabulary, grammar and spelling is called <strong>editing</strong>.</p>\n<p><strong>Revision</strong> is more about semantics than syntax.</p>\n<h2>Writing problems</h2>\n<h3>Grammar</h3>\n<p>Only grammar is not enough to be a good writer, but poor grammar will distract reader from your writing.</p>\n<p>Priorities in grammar:</p>\n<ol>\n<li>Correct vocabulary</li>\n<li>Verb tenses</li>\n<li>Subject-verb agreement</li>\n<li>Plural vs singular.</li>\n</ol>\n<p>Less critical (people should still understand you):</p>\n<ul>\n<li>prepositions</li>\n<li>articles</li>\n</ul>\n<p><em>What is your biggest grammar problem and how to improve it?</em></p>\n<p>I'm not sure yet, as I rarely get feedback on it, except one from the spellchecker. Probably articles and tenses. If you are reading this, could you please give me a comment which explains my biggest grammar problem?</p>\n<h3>Wordiness</h3>\n<p>Reasons for excessive wordiness:</p>\n<ul>\n<li>Trying to sound too formal</li>\n<li>Not knowing more precise vocabulary</li>\n<li>Unnecessary and vague modifiers. Remove words like very, quite, really, etc...</li>\n<li>Use active voice instead of passive</li>\n<li>Remove personal commentary: I believe, I just want to say...</li>\n</ul>\n<h3>Spelling</h3>\n<p>To know how to spell properly is useful even with spellcheckers everywhere. First, spellcheckers don't catch all errors, if misspelled word looks as correctly spelled different word. Second, there are no spellcheckers in handwriting...</p>\n<p>Ways to practice spelling:</p>\n<ul>\n<li>Word games: Scrabble, words with friends...</li>\n<li>Dictation. Also helps with the listening comprehension. You just need to have some correctly written text read aloud slowly enough by someone, type it  into text file, then compare with correct version using diff program like meld (or just regular diff). I just don't know where to find such texts. Audio books would have been nice, but how to reduce their speed and still make them pleasant to listen? Maybe there is some audio player that could make a pause at the end of each sentence said?</li>\n</ul>\n<p>Fun fact: English spelling is hard because it uses alphabet for another language - Latin, which sounded differently.</p>\n<h2>Diction</h2>\n<p><strong>Diction</strong> - the words you chose to express meaning. Your choice should depend on kind of writing you do, and style you selected.</p>\n<p>In English formal words tend to be longer and derive from Latin, while informal - shorter and stem from Anglo-Saxon.</p>\n<p>Common diction errors:</p>\n<ul>\n<li>Among/between Between involves 2 objects, among > 2</li>\n<li>Everyday/Every day. Everyday - adjective = typical. Every day - when something happens.</li>\n<li>Mom,dad/mother,father - formal and informal forms.</li>\n<li>You guys - informal. All of you - formal.</li>\n<li>Infer/imply. Infer is act of thinking. Imply - meaning of something said.</li>\n<li>It's/its. It's - it is. Its - possessive</li>\n</ul>\n<p><strong>Connotation</strong> - idea or feeling that word holds in addition to main meaning (<em>denotation</em>). For example person that is saving money could be cheap, frugal, miserly or economical. Cheap and miserly have negative connotation. Frugal and economical - positive.</p>\n<p><strong>Tone</strong> - emotion of writing: happy, sad, threatening, optimistic. Tone is important as it allows to know authors attitude to the topic.</p>\n<h3>8 ways to develop vocabulary</h3>\n<ol>\n<li>Learn words in categories - kitchen (pot, bowl, ...), bedroom (blanket, pillow, ...)</li>\n<li>Journal. Write down all words that seem important, later use them in sentences.</li>\n<li>Learn collocations (words that often appear together). There are collocation dictionaries online.</li>\n<li>Use mnemonics (like the ones on Memrise)</li>\n<li>Immediately use the word. Write it, speak it.</li>\n<li>Learn area specific vocabulary, use mindmaps to do it.</li>\n<li>Observation journal. Name everything you see. If you didn't know the word - write down in your native language, then lookup.</li>\n<li>Spaced repetition.</li>\n</ol>\n<h2>Homeworks</h2>\n<p>I did not done all the homeworks (as I did not submitted them). I thought that working on writing notes to this course would be enough practice.</p>\n<h3>Taking notes</h3>\n<p><strong>What I Really Want Is Someone Rolling Around in the Text</strong> by <em>Sam Anderson</em></p>\n<p>Author found a book \"How to read a Book\". It seemed old and boring to him, but he remembered only one idea: \"You don't really own a book until you marked it\".</p>\n<p>He quickly adopted a habit of marginalia - making notes on margins, underlying important parts, marking keywords, writing down numbers of important pages at the end. It is common practice of effective study, but for him it felt like a way to more actively interact with text.</p>\n<p>His habit developed to more intense and complex. He has long list of different figures for markings, and it seems like addition. He rarely reads without writing instrument in hand.</p>\n<p>With e-books it is not as easy to make notes on the margins.</p>\n<p>Golden age of marginalia was in 1700-1850. Back then people used books like we now use social networks - to comment on ideas and share with each other. Common gift was an annotated book. There was a person, Samuel Taylor Coleridge, whose friends begged him to annotate his books, later he published his own \"marginalia\" and invented the word \"marginalia\".</p>\n<p>Marginalia could be a bridge between modern internet social networks and literature.</p>\n<p>A friend of author borrowed from him a book, but he needed it so took it back before she finished it. She had to use clean one. After finishing she told that she felt lonely reading unannotated version.</p>\n<p>He imagines putting his notes on transparent margin-sized plastic strips, so he could share them with friends, and receive notes from friends instead. Than he understood that this idea is more easier to implement using modern digital tech.</p>\n<p>Author of blog Book Two argues that the only meaningful thing we could own with digital books is the reading process itself. Because digital books lack this meta-information of bended pages, marks on margins, etc.</p>\n<p>Amazon launched comment sharing on Kindle, between friends. But it would be nice also to be able to access notes some famous authors made on their books.</p>\n<p>Some people don't like marginalia. In digital variant it would be easy to turn it off.</p>\n<p>Such thing could be a Gutenberg-level revolution.</p>\n<p>He once published his marginalia in a magazine, under title \"A year in marginalia\" (for the year of reading). Feedback was positive.</p>\n<p>It seems natural in age of Web 2.0. YouTubers now do reviews of TV Shows, which is analogous to marginalia.</p>\n<p>Criticism now is very high level and thinks about big picture. Author wants \"someone rolling around in test\", more detailed, and more related to reading.</p>\n<p><strong>What was the main idea of the article?</strong>\nMain idea is that writing and reading notes on the margins of the book is fun and useful activity, and it should be more popular, especially with e-books.</p>\n<p><strong>What was your opinion of the article?</strong>\nInteresting article, gives idea about a new software projects, and also to try new way of reading.</p>\n<p><strong>Was there any part of the article you disagreed with?</strong>\nFirst, it will most likely not be Gutenberg-level revolution. If it were - there were already signs of it. Marginalia is not so popular as passive reading (from what I observed).</p>\n<p>Maybe it is possible, but it as hard to design, as to design modern web, for example. Web is built on simple ideas - you have resources, identified by URL, and they could link to each other.</p>\n<h3>About writing</h3>\n<blockquote>\n<p>Write about the type of writing you do. What do you think about writing? Do you enjoy it? What area of writing do you need to improve most?</p>\n</blockquote>\n<p>I did a lot of writing in Ukrainian before. I have a blog about programming and life in general, but I'm not sure if my writing there is good enough, as I don't get a lot of feedback. Well, probably that means that it was not so good, because I did not became as Dorje Batuu. And that guy is obviously great writer. He writes about his work at NASA in a very interesting way. So, I have a lot of room to improve.</p>\n<p>I enjoy writing, but also struggle with it. At my work, we regularly do performance evaluations, and it turns out there bottleneck is no longer my programming ability, but my communication ability. Given that English is not my native language, and at work I write about what I have to write, not about what I want to write, from there I have my struggles. Not that I don't want to write what I'm required to at my work, but I just could not get so passionate about Jira ticket or documentation, as I could be about making some blog post.</p>\n<p>From feedback at my job I understood that I need most improvement in making my writing clear. We don't have any native English speakers there, to cringe over my spelling and grammar, but often I need to rephrase and repeat something that I have written to make people understand.</p>\n<h3>Movie review in different styles</h3>\n<ol>\n<li>Write a formal and objective review of film, 150-250 words.</li>\n<li>Make same review humorous and informal.</li>\n<li>Make it sad and pessimistic.</li>\n</ol>\n<p>\"Good Bye, Lenin!\" is a German comedy-drama movie, events of which happen in Berlin. It is a story of a family divided by the wall, and their reaction to the fall of the wall. Mother of the family who strongly believes into the values of DDR, has accident and falls into a coma. She spends months in hospital without consciousness, and when she wakes up, the wall is gone. Her son tries to hide that fact from her, because that would make her worry to much, and could lead to another accident. His efforts are the source of the humor of this film. All the rest is the drama about how the family tries to live through the time of changes. It is also educational, as it lets viewer to learn the facts about period in German history, and explains German ostalgie (nostalgie for the old times of DDR), without criticizing or mocking it. It features the music of Yann Tiersen, which fits it very much, as it makes you to have fun and in the same time make you sad and nostalgic.</p>\n<p>My wife thinks that we moved to Berlin because I wanted to better immerse myself into the cinematic universe of \"Good Bye, Lenin!\". That could partially be true, as this is one of my favorite movies, I rewatched more, then I have seen main character wake up in the Groundhog Day. So when I walk through the Karl-Marx Allee I think: \"Oh, probably here they lived!\". This feature everything Berlin is - world-famous healthcare in Charite, night clubs in weird facilities, picking up free trash stuff from the street, dumping your trash on the street, IKEA furniture and soft drinks as an vanguard of capitalist conquest of the socialistic city that is ongoing till today. Plot of the film is described by it's title, and if you don't know who or what Lenin is - it is symbol for socialism, the wall and all the bad and good stuff that comes with it. But the main hero, Alex, don't want his mother to know that it's time to say to the wall \"Good bye\". So Alex goes for help to his friend, Denis, who is big fan of the film \"The Matrix\" inferring from his T-Shirt. Together they create their own rea-life version of the Matrix, and put Alex's mother in there. Will she break from the Matrix? I won't tell you to not spoil the film. Watch it for yourself, it is worth the time.</p>\n<p>I'll pass third part of assignment, as I spent a lot of time on it already, and nothing just comes to my mind.</p>"},{"id":"coursera_mt","title":"Machine Translation course notes","date":"2020-12-04","toc":true,"mathjax":true,"tags":["MOOC","notes"],"excerpt":"<p>Recently finished Coursera course on <a href=\"https://www.coursera.org/learn/machinetranslation/\">Machine Translation</a>. That was mostly overwiew course without any programming practice, but with a lot of links to scientific papers. So it is somewhat theoretical, but I would probably recommend if you want to dive into that field from research point of view.</p>\n<p>So for example you get quizzed with questions like:\nOne important difference between phrase-based MT and n-gram based MT is:</p>\n<ol>\n<li>N-gram based MT has a fixed segmentation into units.</li>\n<li>..</li>\n</ol>\n<p>But you will not be taught how to implement any of those. Instead you will get links to <a href=\"http://www.statmt.org/moses/\">MOSES</a> &#x26; <a href=\"http://www.statmt.org/moses/giza/GIZA++.html\">GIZA</a> projects. But probably course will help to understand that project.</p>\n<p>I'm not at all a researcher, have no skill of reading the papers, and to get most of that course, tried to just write down notes. I have couple of promising areas into which I should dive deeper and experiment, to help me with my job of automating content creation for multilingual e-commerce site. And just on listening lectures, writing down notes and doing quizzes took me 14 hours measured in pomodoros to complete (already more than SICP, I should return back to it eventually). Then I'll probably try some hands-on experimentation <a href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\">with TensorFlow</a>.</p>\n<p>So, here are typed notes made of my handwritten notes. They probably will not make much sense for you, especially because I'm to lazy to digitize hand-drawn diagrams, but maybe will help you to figure out what is inside the course and if you need to enroll into it yourself.</p>","content":"<p>Recently finished Coursera course on <a href=\"https://www.coursera.org/learn/machinetranslation/\">Machine Translation</a>. That was mostly overwiew course without any programming practice, but with a lot of links to scientific papers. So it is somewhat theoretical, but I would probably recommend if you want to dive into that field from research point of view.</p>\n<p>So for example you get quizzed with questions like:\nOne important difference between phrase-based MT and n-gram based MT is:</p>\n<ol>\n<li>N-gram based MT has a fixed segmentation into units.</li>\n<li>..</li>\n</ol>\n<p>But you will not be taught how to implement any of those. Instead you will get links to <a href=\"http://www.statmt.org/moses/\">MOSES</a> &#x26; <a href=\"http://www.statmt.org/moses/giza/GIZA++.html\">GIZA</a> projects. But probably course will help to understand that project.</p>\n<p>I'm not at all a researcher, have no skill of reading the papers, and to get most of that course, tried to just write down notes. I have couple of promising areas into which I should dive deeper and experiment, to help me with my job of automating content creation for multilingual e-commerce site. And just on listening lectures, writing down notes and doing quizzes took me 14 hours measured in pomodoros to complete (already more than SICP, I should return back to it eventually). Then I'll probably try some hands-on experimentation <a href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\">with TensorFlow</a>.</p>\n<p>So, here are typed notes made of my handwritten notes. They probably will not make much sense for you, especially because I'm to lazy to digitize hand-drawn diagrams, but maybe will help you to figure out what is inside the course and if you need to enroll into it yourself.</p>\n<h2>History</h2>\n<p>Translation has long history. Longest known parallel corpus of text is Bible, which is translated for thousands of years already.</p>\n<p>People started to think about machine translation (MT for short), just when machines appeared - after WW2.</p>\n<p>There was some \"memorandum\" of Warren Weaver in 1949, where he compared task of MT to task of decryption. And first model of translation was decryption model.</p>\n<p>In 1954 there was a Georgetown experiment, where they translated 19 sentences from russian.</p>\n<p>In 1966 was published ALPAC report, that said that MT failed to give meaningful results, and people mostly gave up on this.</p>\n<p>1977 was the year of first successfull application of MT translation in limited scope - <a href=\"https://en.wikipedia.org/wiki/METEO_System\">METEO system</a> for translating weahter forecasts from French to English. It used rule-based approach.</p>\n<p>At the end of 1980s MT research resumed mostly with efforts by Japan.</p>\n<p>First implementations of statistical machine translation appeared in 1990.</p>\n<p>In 2005 MT have seen public adoption by organizations like Google, MicroSoft, EU Parlament...</p>\n<h2>Approaches</h2>\n<p>We could approach translation with different levels of abstraction (from highekst to lowest):</p>\n<ul>\n<li>Interlingua (encodes meaning, sematnic and pragmatic, but is hard to create, because creator needs to know all languages that will be used)</li>\n<li>Semantic transfer</li>\n<li>Syntactic transfer</li>\n<li>Dictionary transfer/lookup</li>\n</ul>\n<p>This hierarchy is called Vaquois triangle.</p>\n<p>Historically MT was approached with:</p>\n<ol>\n<li>Dictionary lookup (looks weird and is not correct)</li>\n<li>Rule based MT (very laborious and manual)</li>\n<li>Example based translation</li>\n<li>Statistical translation (SMT)</li>\n<li>Neural translation (NMT)</li>\n</ol>\n<p>3 to 5 are data driven, which means we don't need to reprogram system for new translations, just retrain it.</p>\n<p>We could translate from and to different modalities - images, voice, etc. But different modalities bring in their challenges, like the need to infer punctuation from intonation, and need to train speach recognition, performance of which will influence performance of system overall.</p>\n<h2>Data</h2>\n<p>Success of MT is also influenced by availability of language data on the web. Modern MT systems train on gigawords datasets, while average human speaks 0.5 gigawords in a whole lifetime.</p>\n<p>One common source of parallel data is European parlament, which publishes documents in 23 languages of EU.</p>\n<h2>Performance</h2>\n<p>Measuring performance of MT system is additional challenge. You could not just compare translations with unittests, as most of the sentences have multiple correct translations. Additionally apart from correctnes, we need to think about style transfer: formal/informal style, politeness, everyday/academic language etc.</p>\n<p>There is popular scoring scheme for MT systems called BLEU.</p>\n<h2>Use cases</h2>\n<ul>\n<li>assimilation: use MT like Google translate to understand foreign text.</li>\n<li>dissemination: produce text in foreigh language. This use case needs a good quality. But here quality could be improved also by tuning source (reducing ambiguity, etc.)</li>\n</ul>\n<h2>Language</h2>\n<p>Syntax of a language defines it's structure, or how to compose it's elements.</p>\n<p>Semantics defines meaning.</p>\n<p>Basic units of machine translation are usually words. And word segmentation is already a nontrivial problem. Winterschuhe in German is two words or one?</p>\n<p>Morphology adds another challenge to the task of translation. Morphemes are built from stem &#x26; morphems.</p>\n<p>Example Finnish -> English:</p>\n<ul>\n<li>talo -> house</li>\n<li>talossa -> in house</li>\n<li>talossani -> in my house</li>\n<li>talossanikin -> in my house too</li>\n<li>talossanikinko -> in my house too?</li>\n</ul>\n<p>Parts of speech define which role word plays in the sentence. Noun, adjective, verb, etc...</p>\n<p>Grammatical category - property of items within grammar: Tense, Voice, Person, Gender, Number...</p>\n<p>Agreement - when grammatical category of one word influences morphology. For example when noun and adjective should have same gender and number.</p>\n<p>Sentence structure could be described by phrase structure grammar:</p>\n<p>Jane buys a house</p>\n<pre><code>S (sentence)\n├── NP (noun phrase)\n│   └── Noun\n│       └── Jane\n└── VP (verb phrase)\n    ├── Verb\n    │   └── Buys\n    └── NP\n        ├── article\n        │   └── a\n        └── Noun\n            └── house\n</code></pre>\n<p>Semantics has important property of compositionality: meaning of a sentence is composed from meaning of its phrases.</p>\n<p>Lexical semantics - meaning of single words.</p>\n<p>It's difficult to derive semantics, because:</p>\n<ul>\n<li>Words could have multiple meanings\n<ul>\n<li>polysemy: one word could have multiple meanings: interest (in something, or %), bank (of a river, or financial institution).</li>\n<li>homonymy: different words could have the same spelling: can (a verb, expresses ability to do action), can (a noun, for example: can of beans).</li>\n</ul>\n</li>\n<li>Semantics is influenced by context</li>\n<li>It is influenced by structure</li>\n<li>Lexical mismatch. Word in one language could be described by different words in other languages, for example rice in japanese has one word for cooked rice, and another word for a rice plant. In spanish fish is \"pez\" when it is in the water, but \"pezcado\" when it is a dish.</li>\n<li>Morphology also adds information. Lehrer, Lehrerin -> male &#x26; female teacher</li>\n</ul>\n<p>It is easier if source language has more information than target one.</p>\n<ul>\n<li>References\n<ul>\n<li>Co-reference (to something in sentenc or outside)</li>\n<li>Anaphora: he, she, it</li>\n<li>Deictic reference: hree, now, I.</li>\n<li>Reference with synonym: house, buidling...</li>\n</ul>\n</li>\n</ul>\n<p>Example: \"If the baby does not strive on the raw milk, boil it.\"</p>\n<p>\"It\" referes to what?</p>\n<ul>\n<li>a) baby</li>\n<li>b) milk</li>\n</ul>\n<p>Additional difficulty is that language is evolwing and constantly gains new words:</p>\n<ul>\n<li>Constructed: Brexit = Britain + Exit</li>\n<li>Borrowed: downgeloaded (german for downloaded)</li>\n</ul>\n<p>Abbreviations, are another challenge for sematnics.</p>\n<h2>Approaches</h2>\n<p>Rule based MT: humans write rules, computer applies rules to text.</p>\n<p>Corpora based: get big text corpora, apply machine learning.</p>\n<p>Vauquois triangle:</p>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f4/Direct_translation_and_transfer_translation_pyramid.svg\" alt=\"Vauquois triangle\"></p>\n<h3>Rule-based</h3>\n<p>Dictionary based. Does not work between languages where structure of sentence changes much.</p>\n<p>Example of application of dictionary based approach - translation memory. Find similar already translated sentence and show to translator.</p>\n<p>Transfer-based. 3-step approach:</p>\n<ol>\n<li>Analyze source and generate it's abstract representation (parse tree, morphology information etc.)</li>\n<li>Transfer source representation to matching target representation using rules.</li>\n<li>Generate target text from target representation. Ex: (house, plural) -> \"houses\".</li>\n</ol>\n<p>Interlingua - language for pure meaning. Just 2 steps:</p>\n<ol>\n<li>Analyze</li>\n<li>Generate</li>\n</ol>\n<p>Interlingua approach helps in cases of having many language pairs.</p>\n<p>But good interlingua is hard to find and it is done only for limited domains.</p>\n<h3>Corpus-based</h3>\n<p>No more manually written rules. We learn models from data.</p>\n<p>It is usually easier to collect data then to have language experts that could write rules.</p>\n<p>Statistical machine translation:</p>\n<ol>\n<li>Create alternative translations.</li>\n<li>Score them.</li>\n<li>Select one with best score.</li>\n</ol>\n<p>First SMT approaches were direct translations, advanced SMT uses interlingua representations.</p>\n<p>Neural machine translation is also corpus based and is the latest approach developed for now. Has following advantages:</p>\n<ul>\n<li>Automatic learning of intermediate representation, no need to design it.</li>\n<li>More compact representations than in SMT</li>\n<li>Better translation quality</li>\n</ul>\n<h2>Data</h2>\n<p>Is main knowledge source in corpus-based approach.</p>\n<ul>\n<li>Monolingual data (raw text) is available in huge amounts.</li>\n<li>Parallel data: pairs of sentences in 2 languages.</li>\n</ul>\n<p>There are algorithms for sentence alignment for cases where we have parallel data aligned by documents, not by sentences.</p>\n<p>Famous corpuses: TED corpus, EU parlament corpus.</p>\n<p>Notation note: source sentences are marked with letter f (for French) and target with e (for English).</p>\n<p>Data requires preprocessing to make text homohenous.</p>\n<p>For example numbers are replaced with placeholder token, as they could refer to pages, which change from language to language, and we would not want to train on that.</p>\n<p>Tokenization: mainly done by spaces and punctuation for european languages. Special cases are abbrefiations.</p>\n<p>True-casing: some words should be uppercase, some lower-case, but at the beginning of the sentence they should be uppercase. Need to train on true case.</p>\n<h2>Evaluation</h2>\n<p>How to measure quality of translation? It is important, as you could only improve what you could measure.</p>\n<p>Approaches: Human vs automatic.</p>\n<p>Difficulties:</p>\n<ul>\n<li>Multiple translations are correct, we could not just compare strings.</li>\n<li>Small changes could be very important. (\"Let's eat grandma\" vs \"Let's eat, grandma\")</li>\n<li>Evaluation is subjective</li>\n</ul>\n<p>Human evaluation:</p>\n<ul>\n<li>Advantage:\n<ul>\n<li>Gold standard</li>\n</ul>\n</li>\n<li>Disadvantages:\n<ul>\n<li>Subjective</li>\n<li>Expensive</li>\n<li>time consuming</li>\n</ul>\n</li>\n</ul>\n<p>Automatic</p>\n<ul>\n<li>Advantages:\n<ul>\n<li>Cheap</li>\n<li>Fast</li>\n</ul>\n</li>\n<li>Disadvantages:\n<ul>\n<li>still need human reference</li>\n<li>difficult. If we made good evaluation program, maybe just use it to select best translation for us?</li>\n</ul>\n</li>\n</ul>\n<p>Granularity:</p>\n<ul>\n<li>Per sentence?</li>\n<li>Per document?</li>\n</ul>\n<p>Task-based evaluation - is translation good for it's application?</p>\n<p>Aspects of evaluation:</p>\n<ul>\n<li>Adequacy: is translation correct</li>\n<li>Fluency: does it sound unusual in that language?</li>\n</ul>\n<p>Error analysis: where is the source of error? Wrong word, wrong order ... There is special software - BLAST - toolkit for error analysis.</p>\n<h3>Human evaluation</h3>\n<p>Expensive, so we need to minimize effort.</p>\n<p>Stats:</p>\n<ul>\n<li><em>Inter annotator agreement</em> - how different annotators evaluate translations. Study was a 2016 shown that <a href=\"https://en.wikipedia.org/wiki/Cohen%27s_kappa\">Cohen's kappa</a> coefficient (0 - random rankings, 1 - rankings are completely the same) for different annotators is 0.357</li>\n<li><em>Intra annotagor agreement</em> - how same annotator ranks the same translation next time it is shown him: 0.529.</li>\n</ul>\n<p>So, translators tend to disagree about quality not even with others, but with themselves.</p>\n<h4>Approaches</h4>\n<p>Direct assesment:<br>\nGiven: source, translation.<br>\nEvaluate: performance, adequacy, fluency at some scale.</p>\n<p>Ranking:<br>\nGiven number of translations, select best of them, or rank them from best to worst.</p>\n<p>Post editing:<br>\nMeasure time or keystrokes to edit machine translation into correct translation.</p>\n<p>Task-based:<br>\nevaluate translation performance where it will be used. Give students translated text, and then quiz them if they understood it.</p>\n<ul>\n<li>Advantage: this measures overall system performance.</li>\n<li>Disadvantage:\n<ul>\n<li>complex</li>\n<li>other factors, like quality of source, or some aspects of task influence result of evaluation</li>\n</ul>\n</li>\n</ul>\n<h2>BLEU: BiLingual Evaluation Understudy</h2>\n<p>BLEU uses human translated examples from the same source as machine translated, and then checks how good they match.</p>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\">e</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.04em;vertical-align:-0.20500000000000007em;\"></span><span class=\"mord sqrt\"><span class=\"root\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.70022em;\"><span style=\"top:-2.878em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size6 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">4</span></span></span></span></span></span></span></span><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.835em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord text\"><span class=\"mord\">1-gram overlap</span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord text\"><span class=\"mord\">2-gram overlap</span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord text\"><span class=\"mord\">3-gram overlap</span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord text\"><span class=\"mord\">4-gram overlap</span></span></span></span><span style=\"top:-2.795em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg width=\"400em\" height=\"1.08em\" viewBox=\"0 0 400000 1080\" preserveAspectRatio=\"xMinYMin slice\"><path d=\"M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z\"></path></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.20500000000000007em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span></span></span></span></span></p>\n<p>Brevity penalty: <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:6.00004em;vertical-align:-2.75002em;\"></span><span class=\"minner\"><span class=\"mopen\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:3.2500200000000006em;\"><span style=\"top:-1.2999899999999998em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎩</span></span></span><span style=\"top:-1.2949899999999999em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎪</span></span></span><span style=\"top:-1.58999em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎪</span></span></span><span style=\"top:-1.8849900000000002em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎪</span></span></span><span style=\"top:-2.17999em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎪</span></span></span><span style=\"top:-2.2049900000000004em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎪</span></span></span><span style=\"top:-3.1500100000000004em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎨</span></span></span><span style=\"top:-4.29501em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎪</span></span></span><span style=\"top:-4.59001em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎪</span></span></span><span style=\"top:-4.885010000000001em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎪</span></span></span><span style=\"top:-5.180010000000001em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎪</span></span></span><span style=\"top:-5.205010000000001em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎪</span></span></span><span style=\"top:-5.50002em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎧</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.75002em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:3.2313250000000004em;\"><span style=\"top:-5.433975000000001em;\"><span class=\"pstrut\" style=\"height:3.2106500000000002em;\"></span><span class=\"mord\"><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord\">∣</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord\">∣</span></span></span><span style=\"top:-3.9939750000000007em;\"><span class=\"pstrut\" style=\"height:3.2106500000000002em;\"></span><span class=\"mord\"></span></span><span style=\"top:-2.5539750000000008em;\"><span class=\"pstrut\" style=\"height:3.2106500000000002em;\"></span><span class=\"mord\"></span></span><span style=\"top:-0.9113250000000003em;\"><span class=\"pstrut\" style=\"height:3.2106500000000002em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.21065em;\"><span style=\"top:-3.4842000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0377857142857143em;\"><span style=\"top:-2.640785714285714em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">∣</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">M</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mtight\">∣</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.5020714285714285em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">∣</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mtight\">∣</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.5377857142857143em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord\">∣</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">≤</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord\">∣</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.731325em;\"><span></span></span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>BP is necessary to not rank too short translations (which loose information) too highly (as score is higher for shorter MT sentences).</p>\n<h2>Statistical Machine translation</h2>\n<p>Word based SMT. Do not directly translate, find probability of some sentence being a translation. As it is hard to compute probability of a sentence (it is small), we move down to words.</p>\n<p>Lexicon: store possible translations for word. Example:</p>\n<p>Wagen:</p>\n<p>| Word    | Count | Probability |\n|---------|-------|-------------|\n| vehicle | 5000  | 0.5         |\n| car     | 3000  | 0.3         |\n| coach   | 1500  | 0.15        |\n| wagon   | 500   | 0.05        |</p>\n<p>Also we have alignment function from target words to source words (which translates to which). Target words additinallly have NULL to \"translate\" source words that does not exist in target.</p>\n<p>Having lexicon and alignment function, we could implement <a href=\"https://en.wikipedia.org/wiki/IBM_alignment_models\">IBM Model 1</a>. To get alignment function, we could use <a href=\"https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\">expectation-maximization algorithm</a>.</p>\n<h3>Language model</h3>\n<p>Language model is a probability distribution of a sentences in language (how probable that given sentence is generated by native speaker of that language). It tries to model fluency (not accuracy).</p>\n<p>Lots of sentences do not occur in training, but it's good to have P > 0 for them.</p>\n<p><em>Markov assumption</em>: probability of word is approximated by n previous words before it.</p>\n<p>Unknown n-gram => count = 0, P = 0 => P(sentence) = 0. This is bad, so we will need smoothing, where we shift some probability to unseen words.</p>\n<p>One smoothing approach is to count unseen n-grams as occuring once, but that shifts a lot of probability to unknown.</p>\n<p>Long n-grams are more precise, but a lot more sparse. We could use interpolation between long and short n-grams:</p>\n<p>Most common smoothing model used today is modified Kneser-Ney smoothing.</p>\n<h3>Translation</h3>\n<p>Task of translation - to find the most probable translation e, given source sentence f.</p>\n<p>Translation model: <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">e</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mclose\">)</span></span></span></span></span>\nLanguage model: <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">e</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>How to combine two models?</p>\n<p>Noisy channel approach from information theory. Assume source language was distorted into foreign language. Find most probable source message.</p>\n<p>P(f) could be dropped, as it does not change depending on e.</p>\n<p>Problem of this model is that often output is fluent, but not accurate. So we add weights to components to better tune it:</p>\n<p>Such kind of interpolation is called log-linear model. With it we are not restricted to two features, but could have any number of them. And with any weight.</p>\n<p>How to get optimal weights?</p>\n<p>We could train in different stages:</p>\n<ol>\n<li>Learn language model and translation model.</li>\n<li>Tuning: learn weights. Test different weights on validation data.</li>\n</ol>\n<p>Common way to speedup tuning, which could be very slow, because of many parameters &#x26; complex BLEU evaluation, is minimum error training:</p>\n<ol>\n<li>Start from random weights.</li>\n<li>Apply Powell search. For all parameters:</li>\n<li>Fix all parameters except current.</li>\n<li>Find best value for current.</li>\n<li>Save result, and start from 1 until having enough results.</li>\n<li>Select best one.</li>\n</ol>\n<p>Need to evaluate translations using BLEU on whole evaluation set.</p>\n<h3>Phrase based MT</h3>\n<p>Instead of having words as a basic units - use phrases. Phrase is any sequence of words.</p>\n<p>It shold be better, because there is no word-to-word correspondence between languages. For example:</p>\n<ul>\n<li>Idioms: kicked the bucket (en) = biss ins Grass (de)</li>\n<li>Translation of word is context dependent: auf meine Kosten (de) = at my cost (en) => (auf = at), auf meinem Shiff = on my boat => (auf = on)</li>\n</ul>\n<p>We use IBM Models to generate Viterbi alignment.</p>\n<p>After alignment - extract phrase pairs from sentences:</p>\n<p>saw = gesehen haben\nbisher = up till now</p>\n<p>Includes words\nwas = what</p>\n<p>And longer phrases:</p>\n<p>what we seen up till now = was wir bisher gesehen haben</p>\n<p>Now, estimate probability from corpus:</p>\n<p>Up till now:</p>\n<p>| f         | count | p(f|e) |\n|-----------|-------|---------|\n| bisher    | 20    | 20/70   |\n| bis jetzt | 13    | 13/70   |\n| bis heute | 8     | 8/70    |\n| ...       | 29    | 29/70   |</p>\n<p>But with log-linear model we could use multiple features to improve translation. For example, when we seen phrase only once:</p>\n<p>Up till now</p>\n<p>| f         | count | p(f|e) |\n|-----------|-------|---------|\n| bisher    | 1     | 1       |</p>\n<p>Probability is 1, but we should not be so sure. So we could check inverse probability - how often we see \"up till now\" given \"bisher\".</p>\n<p>Challenge of phrase-based MT is reordering. In German, for example verb could be split to be at the end and at the beginning of the sentence. So we could do some preordering first, so source sentence looks more like target one.</p>\n<p>Or - use hierarchical phrases - phrases of phrases.</p>\n<p>Was wir X gesehen haben -> What se saw X</p>\n<p>Another approach - parts of speech language models. Build n-gram model from parts of speech: NOUN VERB NOUN. As there are a lot less parts of speech then words, n-grams could be a lot longer, without having a lot of sparsity. And having longer n-grams helps with ordering.</p>\n<p>Also, cluster-based model could be used - automatically cluster words, when part of speech tagging is not available.</p>\n<h2>Neural networks</h2>\n<ul>\n<li>Perceptron</li>\n<li>Multi layer perceptron</li>\n</ul>\n<p>Size of hidden layers is a hyperparameter.</p>\n<p>Error functions:</p>\n<ul>\n<li>Mean square error:</li>\n</ul>\n<ul>\n<li>Cross entropy:</li>\n</ul>\n<p>o - output, t - desired output (target). x - each example from batch.</p>\n<p>Stochastic gradient descent:</p>\n<ol>\n<li>Randomly take example.</li>\n<li>Calculate error function.</li>\n<li>Calculate gradient.</li>\n<li>Update weihts with gradient.</li>\n</ol>\n<p>Model language using neural net.</p>\n<p>Multilayer perceptron, where input - previous n-1 words, output - next word.</p>\n<p>Problem - no fixed vocabulary. Look at most common words, replace the rest with UNKNOWN token.</p>\n<p>Represent each word by index in frequency table.</p>\n<p>Then use one-hot representation, because we don't want to have \"the\" be more close to \"and\" then to \"a\". In one-hot representation all the words are on the equal distance from each other.</p>\n<p>Word embeding layer - group similar words into one embedding. Automatically learned and has less values then input. Used for each word separately and then output of word embedding for whole sentence is feeded into hidden layer.</p>\n<p>Softmax activation function:</p>\n<p>makes sure that sum of all outputs of layer is 1, and is good for modelling probability.</p>\n<h3>Recurrent neural network language model</h3>\n<p>Good for modelling long dependencies, where n-grams does not work good:</p>\n<p>Ich melde mich ... an<br>\nI register myself ...</p>\n<p>register - melde an</p>\n<p>Hidden state depends on input and previous hidden state output. Always insert one word and predict one next word. Use same one-hot representation with embeddings.</p>\n<p>But it has wanishing gradient problem, backpropagation multiplies derivatives, and for first elements in sequence gradient is very small, so it learns very slowly. We try to fix this problem with special recurrent units: LSTM or GRU.</p>\n<h3>Neural translation models</h3>\n<p><strong>N-gram based NMT approach</strong>:</p>\n<p>Reorder source to be similar to target. Extract translation units. Get pairs of minimal translation units.</p>\n<p>Model probability of translating of \"bisher\" to \"up till now\", given history (previous translation units).</p>\n<p>Challenge - there are a lot more possible translation units then words, as translation units are tuples of words.</p>\n<p><strong>Joint models approach</strong>: Add source context to target language model.</p>\n<p><strong>Discriminative word lexicon approach</strong>:\nPredict target word based on all source words in source sentence. Bag of words representation, one hot is replaced with many-hot.</p>\n<h3>Encoder-decoder model</h3>\n<p>Sequence-to-sequence translation. No need for alignment. Whole system is trained together.</p>\n<p>Source -> <strong>Encoder RNN</strong> -> hidden state of encoder is a sequence representation -> <strong>Decoder RNN</strong> -> Target</p>\n<p>Advantage - it is simple. Disadvantage - has bottleneck - fixed size of sentence representation.</p>\n<p>Note: TensorFlow has <a href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\">tutorial on sequence2sequence translation</a></p>\n<p>Decoding: Beam search. It is like greedy search, where we each time select next most probable word, but here we find n most probable words, then predict from them, then again prune to n best results and coninue until the end.</p>\n<h3>Attention based NMT</h3>\n<p>Designed to overcome bottleneck of fixed representation between encoder &#x26; decoder.</p>\n<p>Have sequence of hidden states.</p>\n<p>Also, run RNN on reversed sentence, and as state depends most on the last input, we will have context with the next words. Compbine forward &#x26; backward representation and you will get representation for part of sentence.</p>\n<p>How to determine which source states to use for given decoder state? Another neural network.</p>\n<p>Conditional GRU. <em>Here it become very confusing to me.</em></p>\n<h3>Training</h3>\n<p>Same model differently randomly initialized will have different performances after training.</p>\n<p><strong>Ensemble of models</strong> - average output of multiple models.</p>\n<p>Advantage: better performance.\nDisadvantages: Training speed, decoding speed.</p>\n<p><strong>Weight averating</strong> - while training save model checkpoints. Error could increase when training for longer time, so it's better to have more models with different errors. Take model with average weights from different checkpoints (but not models from different trainings, will not work).</p>\n<h3>Byte-pair encoding</h3>\n<p>Another way to overcome vocabulary limitation (not fixed size, names, compound words, new words like brexit), except using UNKNOWN token, is to represent all possible words with just n symbols.</p>\n<ol>\n<li>Represent sequence with characters.</li>\n<li>Find most frequent two characters.</li>\n<li>Replace them with new symbol</li>\n<li>Repeat</li>\n</ol>\n<p>Then, rare words will be split into multiple parts.</p>\n<h3>Character-based NMT</h3>\n<p>No word segmentation. Istead of word embeddings - character group embedding. Challenge here is longer sequence length.</p>\n<p><a href=\"https://github.com/rsennrich/subword-nmt\">Subword Neural Machine Translation</a></p>\n<h3>Monolingual data for NMT</h3>\n<p>Again, available in a lot larger amounts. Even if it is better to have parallel data.</p>\n<p>Decoder is similar to RNN language model.</p>\n<ul>\n<li>Train language model separately and combine with some weight in decoder.</li>\n<li>Syntetic data. Get monolingual data of target language machine translated to source, even if not very correctly, and train on it like on ordinary parallel data. It will be trained on good target data, even with incorrect source, so it will train to generate correct text.</li>\n</ul>\n<h3>Multilingual MT</h3>\n<p>There are about 6000 languages. Which gives about 36 millions possible translation directions. There are no parallel data for all this pairs. Parallel data exists mostly with English.</p>\n<p>Use English as interlingua? But it is ambiguous.</p>\n<p>Use language-independent representation with language-dependent encoder-decoder and shared attention mechanism.</p>\n<p>There is research from Facebook on <a href=\"https://github.com/facebookresearch/LASER\">Language-Agnostic SEntence Representations</a>.</p>\n<h3>NMT architectures</h3>\n<p>Popular ones are:</p>\n<ul>\n<li>LSTM</li>\n<li>Transformer architecture with self attention.</li>\n</ul>"}],"pages":1,"archives":[{"url":"/month/2021-01/page/1","id":"2021-01","count":4,"title":"2021-01"},{"url":"/month/2020-12/page/1","id":"2020-12","count":1,"title":"2020-12"},{"url":"/month/2020-09/page/1","id":"2020-09","count":2,"title":"2020-09"},{"url":"/month/2020-08/page/1","id":"2020-08","count":2,"title":"2020-08"},{"url":"/month/2020-07/page/1","id":"2020-07","count":10,"title":"2020-07"},{"url":"/month/2020-06/page/1","id":"2020-06","count":7,"title":"2020-06"},{"url":"/month/2018-12/page/1","id":"2018-12","count":1,"title":"2018-12"}],"topics":[{"url":"/tag/MOOC/page/1","id":"MOOC","title":"MOOC","count":2},{"url":"/tag/SICP/page/1","id":"SICP","title":"SICP","count":20},{"url":"/tag/ideas/page/1","id":"ideas","title":"ideas","count":1},{"url":"/tag/notes/page/1","id":"notes","title":"notes","count":5},{"url":"/tag/on writing/page/1","id":"on writing","title":"on writing","count":3}],"tag":"MOOC"},"__N_SSG":true}