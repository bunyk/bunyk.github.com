<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>Taras Bunyk</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/f969c0c492dd18a0081f.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f969c0c492dd18a0081f.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/_next/static/chunks/webpack-fb76148cfcfb42ca18eb.js" defer=""></script><script src="/_next/static/chunks/framework-2191d16384373197bc0a.js" defer=""></script><script src="/_next/static/chunks/main-89e612c37cd79392e22d.js" defer=""></script><script src="/_next/static/chunks/pages/_app-459ac92bde4bcd5c6050.js" defer=""></script><script src="/_next/static/chunks/762-0f685813c4b6ace6d024.js" defer=""></script><script src="/_next/static/chunks/493-1b1d0a0a11d8c4d6a4cd.js" defer=""></script><script src="/_next/static/chunks/pages/tag/%5Btag%5D/page/%5Bpage%5D-a53b3777b3d6f2750a6c.js" defer=""></script><script src="/_next/static/sMnLAto4YhgRliyLu0bmY/_buildManifest.js" defer=""></script><script src="/_next/static/sMnLAto4YhgRliyLu0bmY/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="MuiContainer-root MuiContainer-maxWidthMd"><div class="MuiToolbar-root MuiToolbar-regular jss2 MuiToolbar-gutters"><h1 class="MuiTypography-root jss3 MuiTypography-h5 MuiTypography-noWrap" aligh="center">Taras Bunyk</h1></div><nav class="MuiToolbar-root MuiToolbar-dense jss4 MuiToolbar-gutters"><a href="/">Home</a><a href="http://bunyk.github.io/mandala/">Snowflake drawing</a><a href="/resume">Resume</a><a href="https://bunyk.wordpress.com/">Old blog (in Ukrainian)</a></nav><div class="MuiGrid-root MuiGrid-container MuiGrid-spacing-xs-5"><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-md-9"><div class="MuiPaper-root MuiCard-root jss1 MuiPaper-elevation1 MuiPaper-rounded"><div class="MuiCardHeader-root"><div class="MuiCardHeader-content"><span class="MuiTypography-root MuiCardHeader-title MuiTypography-h5 MuiTypography-displayBlock">Academic and Business Writing course notes</span><span class="MuiTypography-root MuiCardHeader-subheader MuiTypography-body1 MuiTypography-colorTextSecondary MuiTypography-displayBlock">Published: 2021-01-01 Tags: MOOC, notes, on writing</span></div></div><div class="MuiCardContent-root"><p>This post will contain my notes and assignments from the edX course <a href="https://courses.edx.org/courses/course-v1:BerkeleyX+ColWri2.2x+1T2020/course/">Academic and Business Writing</a>.</p>
<p>I would rearrange notes in order that seems more logical for the writing process, not in the order information was provided in course.</p>
<p>In general writing process could be organized in following steps:</p>
<ol>
<li>Reading. To be a good writer you need to be active reader.</li>
<li>Prewriting. All the stuff you do before you get down to actual writing.</li>
<li>Writing draft, trying to use correct vocabulary.</li>
<li>Revision of that draft to produce next draft with fixed writing problems.</li>
<li>Repeating step 4 again for more low-level improvements of text (editing).</li>
</ol>
<p>This steps and ways to get better at them will be detailed in sections below.</p></div><div class="MuiCardActions-root MuiCardActions-spacing"><a class="MuiButtonBase-root MuiButton-root MuiButton-text" tabindex="0" aria-disabled="false" href="/posts/edx_writing"><span class="MuiButton-label">More</span></a></div></div><div class="MuiPaper-root MuiCard-root jss1 MuiPaper-elevation1 MuiPaper-rounded"><div class="MuiCardHeader-root"><div class="MuiCardHeader-content"><span class="MuiTypography-root MuiCardHeader-title MuiTypography-h5 MuiTypography-displayBlock">Machine Translation course notes</span><span class="MuiTypography-root MuiCardHeader-subheader MuiTypography-body1 MuiTypography-colorTextSecondary MuiTypography-displayBlock">Published: 2020-12-04 Tags: MOOC, notes</span></div></div><div class="MuiCardContent-root"><p>Recently finished Coursera course on <a href="https://www.coursera.org/learn/machinetranslation/">Machine Translation</a>. That was mostly overwiew course without any programming practice, but with a lot of links to scientific papers. So it is somewhat theoretical, but I would probably recommend if you want to dive into that field from research point of view.</p>
<p>So for example you get quizzed with questions like:
One important difference between phrase-based MT and n-gram based MT is:</p>
<ol>
<li>N-gram based MT has a fixed segmentation into units.</li>
<li>..</li>
</ol>
<p>But you will not be taught how to implement any of those. Instead you will get links to <a href="http://www.statmt.org/moses/">MOSES</a> &#x26; <a href="http://www.statmt.org/moses/giza/GIZA++.html">GIZA</a> projects. But probably course will help to understand that project.</p>
<p>I'm not at all a researcher, have no skill of reading the papers, and to get most of that course, tried to just write down notes. I have couple of promising areas into which I should dive deeper and experiment, to help me with my job of automating content creation for multilingual e-commerce site. And just on listening lectures, writing down notes and doing quizzes took me 14 hours measured in pomodoros to complete (already more than SICP, I should return back to it eventually). Then I'll probably try some hands-on experimentation <a href="https://www.tensorflow.org/tutorials/text/nmt_with_attention">with TensorFlow</a>.</p>
<p>So, here are typed notes made of my handwritten notes. They probably will not make much sense for you, especially because I'm to lazy to digitize hand-drawn diagrams, but maybe will help you to figure out what is inside the course and if you need to enroll into it yourself.</p></div><div class="MuiCardActions-root MuiCardActions-spacing"><a class="MuiButtonBase-root MuiButton-root MuiButton-text" tabindex="0" aria-disabled="false" href="/posts/coursera_mt"><span class="MuiButton-label">More</span></a></div></div><button class="MuiButtonBase-root MuiButton-root MuiButton-text Mui-disabled Mui-disabled" tabindex="-1" type="button" disabled=""><span class="MuiButton-label">1</span></button></div><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-md-3"><h6 class="MuiTypography-root jss6 MuiTypography-h6 MuiTypography-gutterBottom">Archives</h6><ul><li><a href="/month/2021-01/page/1">2021-01</a> <span title="posts count">(<!-- -->4<!-- -->)</span></li><li><a href="/month/2020-12/page/1">2020-12</a> <span title="posts count">(<!-- -->1<!-- -->)</span></li><li><a href="/month/2020-09/page/1">2020-09</a> <span title="posts count">(<!-- -->2<!-- -->)</span></li><li><a href="/month/2020-08/page/1">2020-08</a> <span title="posts count">(<!-- -->2<!-- -->)</span></li><li><a href="/month/2020-07/page/1">2020-07</a> <span title="posts count">(<!-- -->10<!-- -->)</span></li><li><a href="/month/2020-06/page/1">2020-06</a> <span title="posts count">(<!-- -->7<!-- -->)</span></li><li><a href="/month/2018-12/page/1">2018-12</a> <span title="posts count">(<!-- -->1<!-- -->)</span></li></ul><h6 class="MuiTypography-root jss6 MuiTypography-h6 MuiTypography-gutterBottom">Topics</h6><ul><li><a href="/tag/MOOC/page/1">MOOC</a> <span title="posts count">(<!-- -->2<!-- -->)</span></li><li><a href="/tag/SICP/page/1">SICP</a> <span title="posts count">(<!-- -->20<!-- -->)</span></li><li><a href="/tag/ideas/page/1">ideas</a> <span title="posts count">(<!-- -->1<!-- -->)</span></li><li><a href="/tag/notes/page/1">notes</a> <span title="posts count">(<!-- -->5<!-- -->)</span></li><li><a href="/tag/on%20writing/page/1">on writing</a> <span title="posts count">(<!-- -->3<!-- -->)</span></li></ul></div><p class="MuiTypography-root MuiTypography-body2 MuiTypography-colorTextSecondary MuiTypography-alignCenter">© <!-- -->2021<!-- --> Bunyk Taras. Built with Material UI &amp; Next.js</p></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"pageNumber":"1","posts":[{"id":"edx_writing","title":"Academic and Business Writing course notes","date":"2021-01-01","toc":true,"tags":["MOOC","notes","on writing"],"excerpt":"\u003cp\u003eThis post will contain my notes and assignments from the edX course \u003ca href=\"https://courses.edx.org/courses/course-v1:BerkeleyX+ColWri2.2x+1T2020/course/\"\u003eAcademic and Business Writing\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI would rearrange notes in order that seems more logical for the writing process, not in the order information was provided in course.\u003c/p\u003e\n\u003cp\u003eIn general writing process could be organized in following steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eReading. To be a good writer you need to be active reader.\u003c/li\u003e\n\u003cli\u003ePrewriting. All the stuff you do before you get down to actual writing.\u003c/li\u003e\n\u003cli\u003eWriting draft, trying to use correct vocabulary.\u003c/li\u003e\n\u003cli\u003eRevision of that draft to produce next draft with fixed writing problems.\u003c/li\u003e\n\u003cli\u003eRepeating step 4 again for more low-level improvements of text (editing).\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis steps and ways to get better at them will be detailed in sections below.\u003c/p\u003e","content":"\u003cp\u003eThis post will contain my notes and assignments from the edX course \u003ca href=\"https://courses.edx.org/courses/course-v1:BerkeleyX+ColWri2.2x+1T2020/course/\"\u003eAcademic and Business Writing\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI would rearrange notes in order that seems more logical for the writing process, not in the order information was provided in course.\u003c/p\u003e\n\u003cp\u003eIn general writing process could be organized in following steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eReading. To be a good writer you need to be active reader.\u003c/li\u003e\n\u003cli\u003ePrewriting. All the stuff you do before you get down to actual writing.\u003c/li\u003e\n\u003cli\u003eWriting draft, trying to use correct vocabulary.\u003c/li\u003e\n\u003cli\u003eRevision of that draft to produce next draft with fixed writing problems.\u003c/li\u003e\n\u003cli\u003eRepeating step 4 again for more low-level improvements of text (editing).\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis steps and ways to get better at them will be detailed in sections below.\u003c/p\u003e\n\u003ch2\u003eReading\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eInterrogate\u003c/strong\u003e - to question closely, aggressively and formally.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eInterrogate text\u003c/strong\u003e - to read it critically, writing down questions. What that means? Why author came to that conclusion?...\u003c/p\u003e\n\u003cp\u003eTo really understand text, write down it's ideas in your own words.\u003c/p\u003e\n\u003cp\u003eMake outline of text.\u003c/p\u003e\n\u003cp\u003eWrite summary after finishing chapter or document. What were main ideas there?\u003c/p\u003e\n\u003cp\u003eAnalyze. What is the evidence provided? Is logic correct?\u003c/p\u003e\n\u003cp\u003eCompare and contrast with other texts. Does it agree or disagree?\u003c/p\u003e\n\u003cp\u003eReading is actually a conversation with an author.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAnnotating\u003c/strong\u003e is almost synonymous with taking notes, but refers to making notes directly in text - underlining/highlighting, writing on margins... You could use post-its to add annotations in book you don't want to damage.\u003c/p\u003e\n\u003cp\u003eAs an exercise for this chapter it was advised to write notes for the article \u003ca href=\"https://www.nytimes.com/2011/03/06/magazine/06Riff-t.html\"\u003eWhat I Really Want Is Someone Rolling Around in the Text\u003c/a\u003e. Here are \u003ca href=\"#taking-notes\"\u003emy notes\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003ePrewriting (informal planning)\u003c/h2\u003e\n\u003cp\u003eWill improve writing quality and save your time on revisiting and rewriting.\u003c/p\u003e\n\u003cp\u003eCommon activities include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eidea maps\u003c/li\u003e\n\u003cli\u003eoutlines\u003c/li\u003e\n\u003cli\u003efreewriting\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBut also following, we will look in more details:\u003c/p\u003e\n\u003ch3\u003eCubing\u003c/h3\u003e\n\u003cp\u003eLook at your topic from 6 points of view (sides of cube):\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eDescribe\u003c/strong\u003e topic. Its shape, size, color, sound... Use all your senses.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCompare\u003c/strong\u003e to related topics. How is it different or similar?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAssociate\u003c/strong\u003e. What does it make you think about?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAnalyze\u003c/strong\u003e its parts. How they connect? Have they equal or different importance?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eApply\u003c/strong\u003e. What could be done with it? What's the use\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArgue\u003c/strong\u003e for and against the topic. How could someone disagree with it?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eThe matrix\u003c/h3\u003e\n\u003cp\u003eMulti-point comparison of parts of topic with questions or points.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThis basically was just formatting facts about your topic in a table. Not sure it will work with all topics, or I did not got it correctly\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003eInterviewing topic\u003c/h3\u003e\n\u003cp\u003eImagine topic is a person and ask it questions:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eYour full name? Does someone know you by different name?\u003c/li\u003e\n\u003cli\u003eHow does dictionary or encyclopedia define you?\u003c/li\u003e\n\u003cli\u003eWhere, how and when you are born?\u003c/li\u003e\n\u003cli\u003eAre you still alive? How you died?\u003c/li\u003e\n\u003cli\u003eTo what group do you belong? Who are others in your group?\u003c/li\u003e\n\u003cli\u003eCan you be divided in parts? How?\u003c/li\u003e\n\u003cli\u003eWere you different in the past? Will you change in the future?\u003c/li\u003e\n\u003cli\u003eWhat's your purpose?\u003c/li\u003e\n\u003cli\u003eWhat is similar or different from you?\u003c/li\u003e\n\u003cli\u003eWhat are some interesting facts or statistics about you?\u003c/li\u003e\n\u003cli\u003eWho could I talk to about you? Are there any experts?\u003c/li\u003e\n\u003cli\u003eQuotes about you?\u003c/li\u003e\n\u003cli\u003eNews stories about you?\n...\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eDrafting and revision\u003c/h2\u003e\n\u003cp\u003eCreating a draft:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWrite outline\u003c/li\u003e\n\u003cli\u003eWrite introduction\u003c/li\u003e\n\u003cli\u003eWrite paragraphs\u003c/li\u003e\n\u003cli\u003eWrite conclusion\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAfter the first draft comes revision(s). You don't focus on grammar and spelling at first, first you reorganize sections, delete unnecessary, add missing, provide additional examples, add clarity to arguments, etc...\u003c/p\u003e\n\u003cp\u003eRevision checklist:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIs title interesting and descriptive?\u003c/li\u003e\n\u003cli\u003eDoes opening catches attention?\u003c/li\u003e\n\u003cli\u003eWill your readers learn something interesting?\u003c/li\u003e\n\u003cli\u003eIs it organized logically?\u003c/li\u003e\n\u003cli\u003eDo you support your ideas with examples?\u003c/li\u003e\n\u003cli\u003eIs there unnecessary information to remove?\u003c/li\u003e\n\u003cli\u003eAre transitions from one idea to next effective?\u003c/li\u003e\n\u003cli\u003eIs conclusion satisfying and leaves the reader with a food for thought?\u003c/li\u003e\n\u003cli\u003eTone and diction\u003c/li\u003e\n\u003cli\u003eVocabulary\u003c/li\u003e\n\u003cli\u003ePunctuation\u003c/li\u003e\n\u003cli\u003eGrammar\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSome tricks to catch errors:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRead the text backwards\u003c/li\u003e\n\u003cli\u003eRead it aloud\u003c/li\u003e\n\u003cli\u003eListen to it using text to speech software.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFixing vocabulary, grammar and spelling is called \u003cstrong\u003eediting\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRevision\u003c/strong\u003e is more about semantics than syntax.\u003c/p\u003e\n\u003ch2\u003eWriting problems\u003c/h2\u003e\n\u003ch3\u003eGrammar\u003c/h3\u003e\n\u003cp\u003eOnly grammar is not enough to be a good writer, but poor grammar will distract reader from your writing.\u003c/p\u003e\n\u003cp\u003ePriorities in grammar:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCorrect vocabulary\u003c/li\u003e\n\u003cli\u003eVerb tenses\u003c/li\u003e\n\u003cli\u003eSubject-verb agreement\u003c/li\u003e\n\u003cli\u003ePlural vs singular.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eLess critical (people should still understand you):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eprepositions\u003c/li\u003e\n\u003cli\u003earticles\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003eWhat is your biggest grammar problem and how to improve it?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eI'm not sure yet, as I rarely get feedback on it, except one from the spellchecker. Probably articles and tenses. If you are reading this, could you please give me a comment which explains my biggest grammar problem?\u003c/p\u003e\n\u003ch3\u003eWordiness\u003c/h3\u003e\n\u003cp\u003eReasons for excessive wordiness:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTrying to sound too formal\u003c/li\u003e\n\u003cli\u003eNot knowing more precise vocabulary\u003c/li\u003e\n\u003cli\u003eUnnecessary and vague modifiers. Remove words like very, quite, really, etc...\u003c/li\u003e\n\u003cli\u003eUse active voice instead of passive\u003c/li\u003e\n\u003cli\u003eRemove personal commentary: I believe, I just want to say...\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eSpelling\u003c/h3\u003e\n\u003cp\u003eTo know how to spell properly is useful even with spellcheckers everywhere. First, spellcheckers don't catch all errors, if misspelled word looks as correctly spelled different word. Second, there are no spellcheckers in handwriting...\u003c/p\u003e\n\u003cp\u003eWays to practice spelling:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWord games: Scrabble, words with friends...\u003c/li\u003e\n\u003cli\u003eDictation. Also helps with the listening comprehension. You just need to have some correctly written text read aloud slowly enough by someone, type it  into text file, then compare with correct version using diff program like meld (or just regular diff). I just don't know where to find such texts. Audio books would have been nice, but how to reduce their speed and still make them pleasant to listen? Maybe there is some audio player that could make a pause at the end of each sentence said?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFun fact: English spelling is hard because it uses alphabet for another language - Latin, which sounded differently.\u003c/p\u003e\n\u003ch2\u003eDiction\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eDiction\u003c/strong\u003e - the words you chose to express meaning. Your choice should depend on kind of writing you do, and style you selected.\u003c/p\u003e\n\u003cp\u003eIn English formal words tend to be longer and derive from Latin, while informal - shorter and stem from Anglo-Saxon.\u003c/p\u003e\n\u003cp\u003eCommon diction errors:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAmong/between Between involves 2 objects, among \u003e 2\u003c/li\u003e\n\u003cli\u003eEveryday/Every day. Everyday - adjective = typical. Every day - when something happens.\u003c/li\u003e\n\u003cli\u003eMom,dad/mother,father - formal and informal forms.\u003c/li\u003e\n\u003cli\u003eYou guys - informal. All of you - formal.\u003c/li\u003e\n\u003cli\u003eInfer/imply. Infer is act of thinking. Imply - meaning of something said.\u003c/li\u003e\n\u003cli\u003eIt's/its. It's - it is. Its - possessive\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eConnotation\u003c/strong\u003e - idea or feeling that word holds in addition to main meaning (\u003cem\u003edenotation\u003c/em\u003e). For example person that is saving money could be cheap, frugal, miserly or economical. Cheap and miserly have negative connotation. Frugal and economical - positive.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTone\u003c/strong\u003e - emotion of writing: happy, sad, threatening, optimistic. Tone is important as it allows to know authors attitude to the topic.\u003c/p\u003e\n\u003ch3\u003e8 ways to develop vocabulary\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eLearn words in categories - kitchen (pot, bowl, ...), bedroom (blanket, pillow, ...)\u003c/li\u003e\n\u003cli\u003eJournal. Write down all words that seem important, later use them in sentences.\u003c/li\u003e\n\u003cli\u003eLearn collocations (words that often appear together). There are collocation dictionaries online.\u003c/li\u003e\n\u003cli\u003eUse mnemonics (like the ones on Memrise)\u003c/li\u003e\n\u003cli\u003eImmediately use the word. Write it, speak it.\u003c/li\u003e\n\u003cli\u003eLearn area specific vocabulary, use mindmaps to do it.\u003c/li\u003e\n\u003cli\u003eObservation journal. Name everything you see. If you didn't know the word - write down in your native language, then lookup.\u003c/li\u003e\n\u003cli\u003eSpaced repetition.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eHomeworks\u003c/h2\u003e\n\u003cp\u003eI did not done all the homeworks (as I did not submitted them). I thought that working on writing notes to this course would be enough practice.\u003c/p\u003e\n\u003ch3\u003eTaking notes\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eWhat I Really Want Is Someone Rolling Around in the Text\u003c/strong\u003e by \u003cem\u003eSam Anderson\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAuthor found a book \"How to read a Book\". It seemed old and boring to him, but he remembered only one idea: \"You don't really own a book until you marked it\".\u003c/p\u003e\n\u003cp\u003eHe quickly adopted a habit of marginalia - making notes on margins, underlying important parts, marking keywords, writing down numbers of important pages at the end. It is common practice of effective study, but for him it felt like a way to more actively interact with text.\u003c/p\u003e\n\u003cp\u003eHis habit developed to more intense and complex. He has long list of different figures for markings, and it seems like addition. He rarely reads without writing instrument in hand.\u003c/p\u003e\n\u003cp\u003eWith e-books it is not as easy to make notes on the margins.\u003c/p\u003e\n\u003cp\u003eGolden age of marginalia was in 1700-1850. Back then people used books like we now use social networks - to comment on ideas and share with each other. Common gift was an annotated book. There was a person, Samuel Taylor Coleridge, whose friends begged him to annotate his books, later he published his own \"marginalia\" and invented the word \"marginalia\".\u003c/p\u003e\n\u003cp\u003eMarginalia could be a bridge between modern internet social networks and literature.\u003c/p\u003e\n\u003cp\u003eA friend of author borrowed from him a book, but he needed it so took it back before she finished it. She had to use clean one. After finishing she told that she felt lonely reading unannotated version.\u003c/p\u003e\n\u003cp\u003eHe imagines putting his notes on transparent margin-sized plastic strips, so he could share them with friends, and receive notes from friends instead. Than he understood that this idea is more easier to implement using modern digital tech.\u003c/p\u003e\n\u003cp\u003eAuthor of blog Book Two argues that the only meaningful thing we could own with digital books is the reading process itself. Because digital books lack this meta-information of bended pages, marks on margins, etc.\u003c/p\u003e\n\u003cp\u003eAmazon launched comment sharing on Kindle, between friends. But it would be nice also to be able to access notes some famous authors made on their books.\u003c/p\u003e\n\u003cp\u003eSome people don't like marginalia. In digital variant it would be easy to turn it off.\u003c/p\u003e\n\u003cp\u003eSuch thing could be a Gutenberg-level revolution.\u003c/p\u003e\n\u003cp\u003eHe once published his marginalia in a magazine, under title \"A year in marginalia\" (for the year of reading). Feedback was positive.\u003c/p\u003e\n\u003cp\u003eIt seems natural in age of Web 2.0. YouTubers now do reviews of TV Shows, which is analogous to marginalia.\u003c/p\u003e\n\u003cp\u003eCriticism now is very high level and thinks about big picture. Author wants \"someone rolling around in test\", more detailed, and more related to reading.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat was the main idea of the article?\u003c/strong\u003e\nMain idea is that writing and reading notes on the margins of the book is fun and useful activity, and it should be more popular, especially with e-books.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat was your opinion of the article?\u003c/strong\u003e\nInteresting article, gives idea about a new software projects, and also to try new way of reading.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWas there any part of the article you disagreed with?\u003c/strong\u003e\nFirst, it will most likely not be Gutenberg-level revolution. If it were - there were already signs of it. Marginalia is not so popular as passive reading (from what I observed).\u003c/p\u003e\n\u003cp\u003eMaybe it is possible, but it as hard to design, as to design modern web, for example. Web is built on simple ideas - you have resources, identified by URL, and they could link to each other.\u003c/p\u003e\n\u003ch3\u003eAbout writing\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWrite about the type of writing you do. What do you think about writing? Do you enjoy it? What area of writing do you need to improve most?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eI did a lot of writing in Ukrainian before. I have a blog about programming and life in general, but I'm not sure if my writing there is good enough, as I don't get a lot of feedback. Well, probably that means that it was not so good, because I did not became as Dorje Batuu. And that guy is obviously great writer. He writes about his work at NASA in a very interesting way. So, I have a lot of room to improve.\u003c/p\u003e\n\u003cp\u003eI enjoy writing, but also struggle with it. At my work, we regularly do performance evaluations, and it turns out there bottleneck is no longer my programming ability, but my communication ability. Given that English is not my native language, and at work I write about what I have to write, not about what I want to write, from there I have my struggles. Not that I don't want to write what I'm required to at my work, but I just could not get so passionate about Jira ticket or documentation, as I could be about making some blog post.\u003c/p\u003e\n\u003cp\u003eFrom feedback at my job I understood that I need most improvement in making my writing clear. We don't have any native English speakers there, to cringe over my spelling and grammar, but often I need to rephrase and repeat something that I have written to make people understand.\u003c/p\u003e\n\u003ch3\u003eMovie review in different styles\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eWrite a formal and objective review of film, 150-250 words.\u003c/li\u003e\n\u003cli\u003eMake same review humorous and informal.\u003c/li\u003e\n\u003cli\u003eMake it sad and pessimistic.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\"Good Bye, Lenin!\" is a German comedy-drama movie, events of which happen in Berlin. It is a story of a family divided by the wall, and their reaction to the fall of the wall. Mother of the family who strongly believes into the values of DDR, has accident and falls into a coma. She spends months in hospital without consciousness, and when she wakes up, the wall is gone. Her son tries to hide that fact from her, because that would make her worry to much, and could lead to another accident. His efforts are the source of the humor of this film. All the rest is the drama about how the family tries to live through the time of changes. It is also educational, as it lets viewer to learn the facts about period in German history, and explains German ostalgie (nostalgie for the old times of DDR), without criticizing or mocking it. It features the music of Yann Tiersen, which fits it very much, as it makes you to have fun and in the same time make you sad and nostalgic.\u003c/p\u003e\n\u003cp\u003eMy wife thinks that we moved to Berlin because I wanted to better immerse myself into the cinematic universe of \"Good Bye, Lenin!\". That could partially be true, as this is one of my favorite movies, I rewatched more, then I have seen main character wake up in the Groundhog Day. So when I walk through the Karl-Marx Allee I think: \"Oh, probably here they lived!\". This feature everything Berlin is - world-famous healthcare in Charite, night clubs in weird facilities, picking up free trash stuff from the street, dumping your trash on the street, IKEA furniture and soft drinks as an vanguard of capitalist conquest of the socialistic city that is ongoing till today. Plot of the film is described by it's title, and if you don't know who or what Lenin is - it is symbol for socialism, the wall and all the bad and good stuff that comes with it. But the main hero, Alex, don't want his mother to know that it's time to say to the wall \"Good bye\". So Alex goes for help to his friend, Denis, who is big fan of the film \"The Matrix\" inferring from his T-Shirt. Together they create their own rea-life version of the Matrix, and put Alex's mother in there. Will she break from the Matrix? I won't tell you to not spoil the film. Watch it for yourself, it is worth the time.\u003c/p\u003e\n\u003cp\u003eI'll pass third part of assignment, as I spent a lot of time on it already, and nothing just comes to my mind.\u003c/p\u003e"},{"id":"coursera_mt","title":"Machine Translation course notes","date":"2020-12-04","toc":true,"mathjax":true,"tags":["MOOC","notes"],"excerpt":"\u003cp\u003eRecently finished Coursera course on \u003ca href=\"https://www.coursera.org/learn/machinetranslation/\"\u003eMachine Translation\u003c/a\u003e. That was mostly overwiew course without any programming practice, but with a lot of links to scientific papers. So it is somewhat theoretical, but I would probably recommend if you want to dive into that field from research point of view.\u003c/p\u003e\n\u003cp\u003eSo for example you get quizzed with questions like:\nOne important difference between phrase-based MT and n-gram based MT is:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eN-gram based MT has a fixed segmentation into units.\u003c/li\u003e\n\u003cli\u003e..\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBut you will not be taught how to implement any of those. Instead you will get links to \u003ca href=\"http://www.statmt.org/moses/\"\u003eMOSES\u003c/a\u003e \u0026#x26; \u003ca href=\"http://www.statmt.org/moses/giza/GIZA++.html\"\u003eGIZA\u003c/a\u003e projects. But probably course will help to understand that project.\u003c/p\u003e\n\u003cp\u003eI'm not at all a researcher, have no skill of reading the papers, and to get most of that course, tried to just write down notes. I have couple of promising areas into which I should dive deeper and experiment, to help me with my job of automating content creation for multilingual e-commerce site. And just on listening lectures, writing down notes and doing quizzes took me 14 hours measured in pomodoros to complete (already more than SICP, I should return back to it eventually). Then I'll probably try some hands-on experimentation \u003ca href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\"\u003ewith TensorFlow\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSo, here are typed notes made of my handwritten notes. They probably will not make much sense for you, especially because I'm to lazy to digitize hand-drawn diagrams, but maybe will help you to figure out what is inside the course and if you need to enroll into it yourself.\u003c/p\u003e","content":"\u003cp\u003eRecently finished Coursera course on \u003ca href=\"https://www.coursera.org/learn/machinetranslation/\"\u003eMachine Translation\u003c/a\u003e. That was mostly overwiew course without any programming practice, but with a lot of links to scientific papers. So it is somewhat theoretical, but I would probably recommend if you want to dive into that field from research point of view.\u003c/p\u003e\n\u003cp\u003eSo for example you get quizzed with questions like:\nOne important difference between phrase-based MT and n-gram based MT is:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eN-gram based MT has a fixed segmentation into units.\u003c/li\u003e\n\u003cli\u003e..\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBut you will not be taught how to implement any of those. Instead you will get links to \u003ca href=\"http://www.statmt.org/moses/\"\u003eMOSES\u003c/a\u003e \u0026#x26; \u003ca href=\"http://www.statmt.org/moses/giza/GIZA++.html\"\u003eGIZA\u003c/a\u003e projects. But probably course will help to understand that project.\u003c/p\u003e\n\u003cp\u003eI'm not at all a researcher, have no skill of reading the papers, and to get most of that course, tried to just write down notes. I have couple of promising areas into which I should dive deeper and experiment, to help me with my job of automating content creation for multilingual e-commerce site. And just on listening lectures, writing down notes and doing quizzes took me 14 hours measured in pomodoros to complete (already more than SICP, I should return back to it eventually). Then I'll probably try some hands-on experimentation \u003ca href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\"\u003ewith TensorFlow\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSo, here are typed notes made of my handwritten notes. They probably will not make much sense for you, especially because I'm to lazy to digitize hand-drawn diagrams, but maybe will help you to figure out what is inside the course and if you need to enroll into it yourself.\u003c/p\u003e\n\u003ch2\u003eHistory\u003c/h2\u003e\n\u003cp\u003eTranslation has long history. Longest known parallel corpus of text is Bible, which is translated for thousands of years already.\u003c/p\u003e\n\u003cp\u003ePeople started to think about machine translation (MT for short), just when machines appeared - after WW2.\u003c/p\u003e\n\u003cp\u003eThere was some \"memorandum\" of Warren Weaver in 1949, where he compared task of MT to task of decryption. And first model of translation was decryption model.\u003c/p\u003e\n\u003cp\u003eIn 1954 there was a Georgetown experiment, where they translated 19 sentences from russian.\u003c/p\u003e\n\u003cp\u003eIn 1966 was published ALPAC report, that said that MT failed to give meaningful results, and people mostly gave up on this.\u003c/p\u003e\n\u003cp\u003e1977 was the year of first successfull application of MT translation in limited scope - \u003ca href=\"https://en.wikipedia.org/wiki/METEO_System\"\u003eMETEO system\u003c/a\u003e for translating weahter forecasts from French to English. It used rule-based approach.\u003c/p\u003e\n\u003cp\u003eAt the end of 1980s MT research resumed mostly with efforts by Japan.\u003c/p\u003e\n\u003cp\u003eFirst implementations of statistical machine translation appeared in 1990.\u003c/p\u003e\n\u003cp\u003eIn 2005 MT have seen public adoption by organizations like Google, MicroSoft, EU Parlament...\u003c/p\u003e\n\u003ch2\u003eApproaches\u003c/h2\u003e\n\u003cp\u003eWe could approach translation with different levels of abstraction (from highekst to lowest):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInterlingua (encodes meaning, sematnic and pragmatic, but is hard to create, because creator needs to know all languages that will be used)\u003c/li\u003e\n\u003cli\u003eSemantic transfer\u003c/li\u003e\n\u003cli\u003eSyntactic transfer\u003c/li\u003e\n\u003cli\u003eDictionary transfer/lookup\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis hierarchy is called Vaquois triangle.\u003c/p\u003e\n\u003cp\u003eHistorically MT was approached with:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDictionary lookup (looks weird and is not correct)\u003c/li\u003e\n\u003cli\u003eRule based MT (very laborious and manual)\u003c/li\u003e\n\u003cli\u003eExample based translation\u003c/li\u003e\n\u003cli\u003eStatistical translation (SMT)\u003c/li\u003e\n\u003cli\u003eNeural translation (NMT)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e3 to 5 are data driven, which means we don't need to reprogram system for new translations, just retrain it.\u003c/p\u003e\n\u003cp\u003eWe could translate from and to different modalities - images, voice, etc. But different modalities bring in their challenges, like the need to infer punctuation from intonation, and need to train speach recognition, performance of which will influence performance of system overall.\u003c/p\u003e\n\u003ch2\u003eData\u003c/h2\u003e\n\u003cp\u003eSuccess of MT is also influenced by availability of language data on the web. Modern MT systems train on gigawords datasets, while average human speaks 0.5 gigawords in a whole lifetime.\u003c/p\u003e\n\u003cp\u003eOne common source of parallel data is European parlament, which publishes documents in 23 languages of EU.\u003c/p\u003e\n\u003ch2\u003ePerformance\u003c/h2\u003e\n\u003cp\u003eMeasuring performance of MT system is additional challenge. You could not just compare translations with unittests, as most of the sentences have multiple correct translations. Additionally apart from correctnes, we need to think about style transfer: formal/informal style, politeness, everyday/academic language etc.\u003c/p\u003e\n\u003cp\u003eThere is popular scoring scheme for MT systems called BLEU.\u003c/p\u003e\n\u003ch2\u003eUse cases\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eassimilation: use MT like Google translate to understand foreign text.\u003c/li\u003e\n\u003cli\u003edissemination: produce text in foreigh language. This use case needs a good quality. But here quality could be improved also by tuning source (reducing ambiguity, etc.)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eLanguage\u003c/h2\u003e\n\u003cp\u003eSyntax of a language defines it's structure, or how to compose it's elements.\u003c/p\u003e\n\u003cp\u003eSemantics defines meaning.\u003c/p\u003e\n\u003cp\u003eBasic units of machine translation are usually words. And word segmentation is already a nontrivial problem. Winterschuhe in German is two words or one?\u003c/p\u003e\n\u003cp\u003eMorphology adds another challenge to the task of translation. Morphemes are built from stem \u0026#x26; morphems.\u003c/p\u003e\n\u003cp\u003eExample Finnish -\u003e English:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etalo -\u003e house\u003c/li\u003e\n\u003cli\u003etalossa -\u003e in house\u003c/li\u003e\n\u003cli\u003etalossani -\u003e in my house\u003c/li\u003e\n\u003cli\u003etalossanikin -\u003e in my house too\u003c/li\u003e\n\u003cli\u003etalossanikinko -\u003e in my house too?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eParts of speech define which role word plays in the sentence. Noun, adjective, verb, etc...\u003c/p\u003e\n\u003cp\u003eGrammatical category - property of items within grammar: Tense, Voice, Person, Gender, Number...\u003c/p\u003e\n\u003cp\u003eAgreement - when grammatical category of one word influences morphology. For example when noun and adjective should have same gender and number.\u003c/p\u003e\n\u003cp\u003eSentence structure could be described by phrase structure grammar:\u003c/p\u003e\n\u003cp\u003eJane buys a house\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eS (sentence)\n├── NP (noun phrase)\n│   └── Noun\n│       └── Jane\n└── VP (verb phrase)\n    ├── Verb\n    │   └── Buys\n    └── NP\n        ├── article\n        │   └── a\n        └── Noun\n            └── house\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSemantics has important property of compositionality: meaning of a sentence is composed from meaning of its phrases.\u003c/p\u003e\n\u003cp\u003eLexical semantics - meaning of single words.\u003c/p\u003e\n\u003cp\u003eIt's difficult to derive semantics, because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWords could have multiple meanings\n\u003cul\u003e\n\u003cli\u003epolysemy: one word could have multiple meanings: interest (in something, or %), bank (of a river, or financial institution).\u003c/li\u003e\n\u003cli\u003ehomonymy: different words could have the same spelling: can (a verb, expresses ability to do action), can (a noun, for example: can of beans).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSemantics is influenced by context\u003c/li\u003e\n\u003cli\u003eIt is influenced by structure\u003c/li\u003e\n\u003cli\u003eLexical mismatch. Word in one language could be described by different words in other languages, for example rice in japanese has one word for cooked rice, and another word for a rice plant. In spanish fish is \"pez\" when it is in the water, but \"pezcado\" when it is a dish.\u003c/li\u003e\n\u003cli\u003eMorphology also adds information. Lehrer, Lehrerin -\u003e male \u0026#x26; female teacher\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt is easier if source language has more information than target one.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eReferences\n\u003cul\u003e\n\u003cli\u003eCo-reference (to something in sentenc or outside)\u003c/li\u003e\n\u003cli\u003eAnaphora: he, she, it\u003c/li\u003e\n\u003cli\u003eDeictic reference: hree, now, I.\u003c/li\u003e\n\u003cli\u003eReference with synonym: house, buidling...\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExample: \"If the baby does not strive on the raw milk, boil it.\"\u003c/p\u003e\n\u003cp\u003e\"It\" referes to what?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea) baby\u003c/li\u003e\n\u003cli\u003eb) milk\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAdditional difficulty is that language is evolwing and constantly gains new words:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConstructed: Brexit = Britain + Exit\u003c/li\u003e\n\u003cli\u003eBorrowed: downgeloaded (german for downloaded)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAbbreviations, are another challenge for sematnics.\u003c/p\u003e\n\u003ch2\u003eApproaches\u003c/h2\u003e\n\u003cp\u003eRule based MT: humans write rules, computer applies rules to text.\u003c/p\u003e\n\u003cp\u003eCorpora based: get big text corpora, apply machine learning.\u003c/p\u003e\n\u003cp\u003eVauquois triangle:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://upload.wikimedia.org/wikipedia/commons/f/f4/Direct_translation_and_transfer_translation_pyramid.svg\" alt=\"Vauquois triangle\"\u003e\u003c/p\u003e\n\u003ch3\u003eRule-based\u003c/h3\u003e\n\u003cp\u003eDictionary based. Does not work between languages where structure of sentence changes much.\u003c/p\u003e\n\u003cp\u003eExample of application of dictionary based approach - translation memory. Find similar already translated sentence and show to translator.\u003c/p\u003e\n\u003cp\u003eTransfer-based. 3-step approach:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnalyze source and generate it's abstract representation (parse tree, morphology information etc.)\u003c/li\u003e\n\u003cli\u003eTransfer source representation to matching target representation using rules.\u003c/li\u003e\n\u003cli\u003eGenerate target text from target representation. Ex: (house, plural) -\u003e \"houses\".\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eInterlingua - language for pure meaning. Just 2 steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnalyze\u003c/li\u003e\n\u003cli\u003eGenerate\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eInterlingua approach helps in cases of having many language pairs.\u003c/p\u003e\n\u003cp\u003eBut good interlingua is hard to find and it is done only for limited domains.\u003c/p\u003e\n\u003ch3\u003eCorpus-based\u003c/h3\u003e\n\u003cp\u003eNo more manually written rules. We learn models from data.\u003c/p\u003e\n\u003cp\u003eIt is usually easier to collect data then to have language experts that could write rules.\u003c/p\u003e\n\u003cp\u003eStatistical machine translation:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCreate alternative translations.\u003c/li\u003e\n\u003cli\u003eScore them.\u003c/li\u003e\n\u003cli\u003eSelect one with best score.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eFirst SMT approaches were direct translations, advanced SMT uses interlingua representations.\u003c/p\u003e\n\u003cp\u003eNeural machine translation is also corpus based and is the latest approach developed for now. Has following advantages:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAutomatic learning of intermediate representation, no need to design it.\u003c/li\u003e\n\u003cli\u003eMore compact representations than in SMT\u003c/li\u003e\n\u003cli\u003eBetter translation quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eData\u003c/h2\u003e\n\u003cp\u003eIs main knowledge source in corpus-based approach.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMonolingual data (raw text) is available in huge amounts.\u003c/li\u003e\n\u003cli\u003eParallel data: pairs of sentences in 2 languages.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThere are algorithms for sentence alignment for cases where we have parallel data aligned by documents, not by sentences.\u003c/p\u003e\n\u003cp\u003eFamous corpuses: TED corpus, EU parlament corpus.\u003c/p\u003e\n\u003cp\u003eNotation note: source sentences are marked with letter f (for French) and target with e (for English).\u003c/p\u003e\n\u003cp\u003eData requires preprocessing to make text homohenous.\u003c/p\u003e\n\u003cp\u003eFor example numbers are replaced with placeholder token, as they could refer to pages, which change from language to language, and we would not want to train on that.\u003c/p\u003e\n\u003cp\u003eTokenization: mainly done by spaces and punctuation for european languages. Special cases are abbrefiations.\u003c/p\u003e\n\u003cp\u003eTrue-casing: some words should be uppercase, some lower-case, but at the beginning of the sentence they should be uppercase. Need to train on true case.\u003c/p\u003e\n\u003ch2\u003eEvaluation\u003c/h2\u003e\n\u003cp\u003eHow to measure quality of translation? It is important, as you could only improve what you could measure.\u003c/p\u003e\n\u003cp\u003eApproaches: Human vs automatic.\u003c/p\u003e\n\u003cp\u003eDifficulties:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMultiple translations are correct, we could not just compare strings.\u003c/li\u003e\n\u003cli\u003eSmall changes could be very important. (\"Let's eat grandma\" vs \"Let's eat, grandma\")\u003c/li\u003e\n\u003cli\u003eEvaluation is subjective\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHuman evaluation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvantage:\n\u003cul\u003e\n\u003cli\u003eGold standard\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDisadvantages:\n\u003cul\u003e\n\u003cli\u003eSubjective\u003c/li\u003e\n\u003cli\u003eExpensive\u003c/li\u003e\n\u003cli\u003etime consuming\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAutomatic\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvantages:\n\u003cul\u003e\n\u003cli\u003eCheap\u003c/li\u003e\n\u003cli\u003eFast\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDisadvantages:\n\u003cul\u003e\n\u003cli\u003estill need human reference\u003c/li\u003e\n\u003cli\u003edifficult. If we made good evaluation program, maybe just use it to select best translation for us?\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eGranularity:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePer sentence?\u003c/li\u003e\n\u003cli\u003ePer document?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTask-based evaluation - is translation good for it's application?\u003c/p\u003e\n\u003cp\u003eAspects of evaluation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdequacy: is translation correct\u003c/li\u003e\n\u003cli\u003eFluency: does it sound unusual in that language?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eError analysis: where is the source of error? Wrong word, wrong order ... There is special software - BLAST - toolkit for error analysis.\u003c/p\u003e\n\u003ch3\u003eHuman evaluation\u003c/h3\u003e\n\u003cp\u003eExpensive, so we need to minimize effort.\u003c/p\u003e\n\u003cp\u003eStats:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eInter annotator agreement\u003c/em\u003e - how different annotators evaluate translations. Study was a 2016 shown that \u003ca href=\"https://en.wikipedia.org/wiki/Cohen%27s_kappa\"\u003eCohen's kappa\u003c/a\u003e coefficient (0 - random rankings, 1 - rankings are completely the same) for different annotators is 0.357\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eIntra annotagor agreement\u003c/em\u003e - how same annotator ranks the same translation next time it is shown him: 0.529.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSo, translators tend to disagree about quality not even with others, but with themselves.\u003c/p\u003e\n\u003ch4\u003eApproaches\u003c/h4\u003e\n\u003cp\u003eDirect assesment:\u003cbr\u003e\nGiven: source, translation.\u003cbr\u003e\nEvaluate: performance, adequacy, fluency at some scale.\u003c/p\u003e\n\u003cp\u003eRanking:\u003cbr\u003e\nGiven number of translations, select best of them, or rank them from best to worst.\u003c/p\u003e\n\u003cp\u003ePost editing:\u003cbr\u003e\nMeasure time or keystrokes to edit machine translation into correct translation.\u003c/p\u003e\n\u003cp\u003eTask-based:\u003cbr\u003e\nevaluate translation performance where it will be used. Give students translated text, and then quiz them if they understood it.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvantage: this measures overall system performance.\u003c/li\u003e\n\u003cli\u003eDisadvantage:\n\u003cul\u003e\n\u003cli\u003ecomplex\u003c/li\u003e\n\u003cli\u003eother factors, like quality of source, or some aspects of task influence result of evaluation\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eBLEU: BiLingual Evaluation Understudy\u003c/h2\u003e\n\u003cp\u003eBLEU uses human translated examples from the same source as machine translated, and then checks how good they match.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.05764em;\"\u003eS\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ec\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eo\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003er\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.04em;vertical-align:-0.20500000000000007em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord sqrt\"\u003e\u003cspan class=\"root\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.70022em;\"\u003e\u003cspan style=\"top:-2.878em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.5em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size1 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e4\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.835em;\"\u003e\u003cspan class=\"svg-align\" style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\" style=\"padding-left:0.833em;\"\u003e\u003cspan class=\"mord text\"\u003e\u003cspan class=\"mord\"\u003e1-gram overlap\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord text\"\u003e\u003cspan class=\"mord\"\u003e2-gram overlap\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord text\"\u003e\u003cspan class=\"mord\"\u003e3-gram overlap\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord text\"\u003e\u003cspan class=\"mord\"\u003e4-gram overlap\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-2.795em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"\u003e\u003csvg width=\"400em\" height=\"1.08em\" viewBox=\"0 0 400000 1080\" preserveAspectRatio=\"xMinYMin slice\"\u003e\u003cpath d=\"M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.20500000000000007em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.05017em;\"\u003eB\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eP\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eBrevity penalty: \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.05017em;\"\u003eB\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eP\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:6.00004em;vertical-align:-2.75002em;\"\u003e\u003c/span\u003e\u003cspan class=\"minner\"\u003e\u003cspan class=\"mopen\"\u003e\u003cspan class=\"delimsizing mult\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:3.2500200000000006em;\"\u003e\u003cspan style=\"top:-1.2999899999999998em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎩\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-1.2949899999999999em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎪\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-1.58999em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎪\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-1.8849900000000002em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎪\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-2.17999em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎪\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-2.2049900000000004em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎪\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.1500100000000004em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎨\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-4.29501em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎪\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-4.59001em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎪\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-4.885010000000001em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎪\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-5.180010000000001em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎪\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-5.205010000000001em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎪\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-5.50002em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎧\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:2.75002em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mtable\"\u003e\u003cspan class=\"col-align-l\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:3.2313250000000004em;\"\u003e\u003cspan style=\"top:-5.433975000000001em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.2106500000000002em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\"\u003eM\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.08125em;\"\u003eH\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.9939750000000007em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.2106500000000002em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-2.5539750000000008em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.2106500000000002em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-0.9113250000000003em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.2106500000000002em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:1.21065em;\"\u003e\u003cspan style=\"top:-3.4842000000000004em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mopen nulldelimiter sizing reset-size3 size6\"\u003e\u003c/span\u003e\u003cspan class=\"mfrac\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:1.0377857142857143em;\"\u003e\u003cspan style=\"top:-2.640785714285714em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size3 size1 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e∣\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\"\u003eM\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e∣\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.2255000000000003em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.5020714285714285em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size3 size1 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e∣\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.08125em;\"\u003eH\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e∣\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.5377857142857143em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose nulldelimiter sizing reset-size3 size6\"\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\"\u003eM\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e≤\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.08125em;\"\u003eH\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:2.731325em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose nulldelimiter\"\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eBP is necessary to not rank too short translations (which loose information) too highly (as score is higher for shorter MT sentences).\u003c/p\u003e\n\u003ch2\u003eStatistical Machine translation\u003c/h2\u003e\n\u003cp\u003eWord based SMT. Do not directly translate, find probability of some sentence being a translation. As it is hard to compute probability of a sentence (it is small), we move down to words.\u003c/p\u003e\n\u003cp\u003eLexicon: store possible translations for word. Example:\u003c/p\u003e\n\u003cp\u003eWagen:\u003c/p\u003e\n\u003cp\u003e| Word    | Count | Probability |\n|---------|-------|-------------|\n| vehicle | 5000  | 0.5         |\n| car     | 3000  | 0.3         |\n| coach   | 1500  | 0.15        |\n| wagon   | 500   | 0.05        |\u003c/p\u003e\n\u003cp\u003eAlso we have alignment function from target words to source words (which translates to which). Target words additinallly have NULL to \"translate\" source words that does not exist in target.\u003c/p\u003e\n\u003cp\u003eHaving lexicon and alignment function, we could implement \u003ca href=\"https://en.wikipedia.org/wiki/IBM_alignment_models\"\u003eIBM Model 1\u003c/a\u003e. To get alignment function, we could use \u003ca href=\"https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\"\u003eexpectation-maximization algorithm\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003eLanguage model\u003c/h3\u003e\n\u003cp\u003eLanguage model is a probability distribution of a sentences in language (how probable that given sentence is generated by native speaker of that language). It tries to model fluency (not accuracy).\u003c/p\u003e\n\u003cp\u003eLots of sentences do not occur in training, but it's good to have P \u003e 0 for them.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMarkov assumption\u003c/em\u003e: probability of word is approximated by n previous words before it.\u003c/p\u003e\n\u003cp\u003eUnknown n-gram =\u003e count = 0, P = 0 =\u003e P(sentence) = 0. This is bad, so we will need smoothing, where we shift some probability to unseen words.\u003c/p\u003e\n\u003cp\u003eOne smoothing approach is to count unseen n-grams as occuring once, but that shifts a lot of probability to unknown.\u003c/p\u003e\n\u003cp\u003eLong n-grams are more precise, but a lot more sparse. We could use interpolation between long and short n-grams:\u003c/p\u003e\n\u003cp\u003eMost common smoothing model used today is modified Kneser-Ney smoothing.\u003c/p\u003e\n\u003ch3\u003eTranslation\u003c/h3\u003e\n\u003cp\u003eTask of translation - to find the most probable translation e, given source sentence f.\u003c/p\u003e\n\u003cp\u003eTranslation model: \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eP\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003ef\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nLanguage model: \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eP\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eHow to combine two models?\u003c/p\u003e\n\u003cp\u003eNoisy channel approach from information theory. Assume source language was distorted into foreign language. Find most probable source message.\u003c/p\u003e\n\u003cp\u003eP(f) could be dropped, as it does not change depending on e.\u003c/p\u003e\n\u003cp\u003eProblem of this model is that often output is fluent, but not accurate. So we add weights to components to better tune it:\u003c/p\u003e\n\u003cp\u003eSuch kind of interpolation is called log-linear model. With it we are not restricted to two features, but could have any number of them. And with any weight.\u003c/p\u003e\n\u003cp\u003eHow to get optimal weights?\u003c/p\u003e\n\u003cp\u003eWe could train in different stages:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLearn language model and translation model.\u003c/li\u003e\n\u003cli\u003eTuning: learn weights. Test different weights on validation data.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCommon way to speedup tuning, which could be very slow, because of many parameters \u0026#x26; complex BLEU evaluation, is minimum error training:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eStart from random weights.\u003c/li\u003e\n\u003cli\u003eApply Powell search. For all parameters:\u003c/li\u003e\n\u003cli\u003eFix all parameters except current.\u003c/li\u003e\n\u003cli\u003eFind best value for current.\u003c/li\u003e\n\u003cli\u003eSave result, and start from 1 until having enough results.\u003c/li\u003e\n\u003cli\u003eSelect best one.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNeed to evaluate translations using BLEU on whole evaluation set.\u003c/p\u003e\n\u003ch3\u003ePhrase based MT\u003c/h3\u003e\n\u003cp\u003eInstead of having words as a basic units - use phrases. Phrase is any sequence of words.\u003c/p\u003e\n\u003cp\u003eIt shold be better, because there is no word-to-word correspondence between languages. For example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIdioms: kicked the bucket (en) = biss ins Grass (de)\u003c/li\u003e\n\u003cli\u003eTranslation of word is context dependent: auf meine Kosten (de) = at my cost (en) =\u003e (auf = at), auf meinem Shiff = on my boat =\u003e (auf = on)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe use IBM Models to generate Viterbi alignment.\u003c/p\u003e\n\u003cp\u003eAfter alignment - extract phrase pairs from sentences:\u003c/p\u003e\n\u003cp\u003esaw = gesehen haben\nbisher = up till now\u003c/p\u003e\n\u003cp\u003eIncludes words\nwas = what\u003c/p\u003e\n\u003cp\u003eAnd longer phrases:\u003c/p\u003e\n\u003cp\u003ewhat we seen up till now = was wir bisher gesehen haben\u003c/p\u003e\n\u003cp\u003eNow, estimate probability from corpus:\u003c/p\u003e\n\u003cp\u003eUp till now:\u003c/p\u003e\n\u003cp\u003e| f         | count | p(f|e) |\n|-----------|-------|---------|\n| bisher    | 20    | 20/70   |\n| bis jetzt | 13    | 13/70   |\n| bis heute | 8     | 8/70    |\n| ...       | 29    | 29/70   |\u003c/p\u003e\n\u003cp\u003eBut with log-linear model we could use multiple features to improve translation. For example, when we seen phrase only once:\u003c/p\u003e\n\u003cp\u003eUp till now\u003c/p\u003e\n\u003cp\u003e| f         | count | p(f|e) |\n|-----------|-------|---------|\n| bisher    | 1     | 1       |\u003c/p\u003e\n\u003cp\u003eProbability is 1, but we should not be so sure. So we could check inverse probability - how often we see \"up till now\" given \"bisher\".\u003c/p\u003e\n\u003cp\u003eChallenge of phrase-based MT is reordering. In German, for example verb could be split to be at the end and at the beginning of the sentence. So we could do some preordering first, so source sentence looks more like target one.\u003c/p\u003e\n\u003cp\u003eOr - use hierarchical phrases - phrases of phrases.\u003c/p\u003e\n\u003cp\u003eWas wir X gesehen haben -\u003e What se saw X\u003c/p\u003e\n\u003cp\u003eAnother approach - parts of speech language models. Build n-gram model from parts of speech: NOUN VERB NOUN. As there are a lot less parts of speech then words, n-grams could be a lot longer, without having a lot of sparsity. And having longer n-grams helps with ordering.\u003c/p\u003e\n\u003cp\u003eAlso, cluster-based model could be used - automatically cluster words, when part of speech tagging is not available.\u003c/p\u003e\n\u003ch2\u003eNeural networks\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePerceptron\u003c/li\u003e\n\u003cli\u003eMulti layer perceptron\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSize of hidden layers is a hyperparameter.\u003c/p\u003e\n\u003cp\u003eError functions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMean square error:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eCross entropy:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eo - output, t - desired output (target). x - each example from batch.\u003c/p\u003e\n\u003cp\u003eStochastic gradient descent:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRandomly take example.\u003c/li\u003e\n\u003cli\u003eCalculate error function.\u003c/li\u003e\n\u003cli\u003eCalculate gradient.\u003c/li\u003e\n\u003cli\u003eUpdate weihts with gradient.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eModel language using neural net.\u003c/p\u003e\n\u003cp\u003eMultilayer perceptron, where input - previous n-1 words, output - next word.\u003c/p\u003e\n\u003cp\u003eProblem - no fixed vocabulary. Look at most common words, replace the rest with UNKNOWN token.\u003c/p\u003e\n\u003cp\u003eRepresent each word by index in frequency table.\u003c/p\u003e\n\u003cp\u003eThen use one-hot representation, because we don't want to have \"the\" be more close to \"and\" then to \"a\". In one-hot representation all the words are on the equal distance from each other.\u003c/p\u003e\n\u003cp\u003eWord embeding layer - group similar words into one embedding. Automatically learned and has less values then input. Used for each word separately and then output of word embedding for whole sentence is feeded into hidden layer.\u003c/p\u003e\n\u003cp\u003eSoftmax activation function:\u003c/p\u003e\n\u003cp\u003emakes sure that sum of all outputs of layer is 1, and is good for modelling probability.\u003c/p\u003e\n\u003ch3\u003eRecurrent neural network language model\u003c/h3\u003e\n\u003cp\u003eGood for modelling long dependencies, where n-grams does not work good:\u003c/p\u003e\n\u003cp\u003eIch melde mich ... an\u003cbr\u003e\nI register myself ...\u003c/p\u003e\n\u003cp\u003eregister - melde an\u003c/p\u003e\n\u003cp\u003eHidden state depends on input and previous hidden state output. Always insert one word and predict one next word. Use same one-hot representation with embeddings.\u003c/p\u003e\n\u003cp\u003eBut it has wanishing gradient problem, backpropagation multiplies derivatives, and for first elements in sequence gradient is very small, so it learns very slowly. We try to fix this problem with special recurrent units: LSTM or GRU.\u003c/p\u003e\n\u003ch3\u003eNeural translation models\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eN-gram based NMT approach\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eReorder source to be similar to target. Extract translation units. Get pairs of minimal translation units.\u003c/p\u003e\n\u003cp\u003eModel probability of translating of \"bisher\" to \"up till now\", given history (previous translation units).\u003c/p\u003e\n\u003cp\u003eChallenge - there are a lot more possible translation units then words, as translation units are tuples of words.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eJoint models approach\u003c/strong\u003e: Add source context to target language model.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDiscriminative word lexicon approach\u003c/strong\u003e:\nPredict target word based on all source words in source sentence. Bag of words representation, one hot is replaced with many-hot.\u003c/p\u003e\n\u003ch3\u003eEncoder-decoder model\u003c/h3\u003e\n\u003cp\u003eSequence-to-sequence translation. No need for alignment. Whole system is trained together.\u003c/p\u003e\n\u003cp\u003eSource -\u003e \u003cstrong\u003eEncoder RNN\u003c/strong\u003e -\u003e hidden state of encoder is a sequence representation -\u003e \u003cstrong\u003eDecoder RNN\u003c/strong\u003e -\u003e Target\u003c/p\u003e\n\u003cp\u003eAdvantage - it is simple. Disadvantage - has bottleneck - fixed size of sentence representation.\u003c/p\u003e\n\u003cp\u003eNote: TensorFlow has \u003ca href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\"\u003etutorial on sequence2sequence translation\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDecoding: Beam search. It is like greedy search, where we each time select next most probable word, but here we find n most probable words, then predict from them, then again prune to n best results and coninue until the end.\u003c/p\u003e\n\u003ch3\u003eAttention based NMT\u003c/h3\u003e\n\u003cp\u003eDesigned to overcome bottleneck of fixed representation between encoder \u0026#x26; decoder.\u003c/p\u003e\n\u003cp\u003eHave sequence of hidden states.\u003c/p\u003e\n\u003cp\u003eAlso, run RNN on reversed sentence, and as state depends most on the last input, we will have context with the next words. Compbine forward \u0026#x26; backward representation and you will get representation for part of sentence.\u003c/p\u003e\n\u003cp\u003eHow to determine which source states to use for given decoder state? Another neural network.\u003c/p\u003e\n\u003cp\u003eConditional GRU. \u003cem\u003eHere it become very confusing to me.\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003eTraining\u003c/h3\u003e\n\u003cp\u003eSame model differently randomly initialized will have different performances after training.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEnsemble of models\u003c/strong\u003e - average output of multiple models.\u003c/p\u003e\n\u003cp\u003eAdvantage: better performance.\nDisadvantages: Training speed, decoding speed.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWeight averating\u003c/strong\u003e - while training save model checkpoints. Error could increase when training for longer time, so it's better to have more models with different errors. Take model with average weights from different checkpoints (but not models from different trainings, will not work).\u003c/p\u003e\n\u003ch3\u003eByte-pair encoding\u003c/h3\u003e\n\u003cp\u003eAnother way to overcome vocabulary limitation (not fixed size, names, compound words, new words like brexit), except using UNKNOWN token, is to represent all possible words with just n symbols.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRepresent sequence with characters.\u003c/li\u003e\n\u003cli\u003eFind most frequent two characters.\u003c/li\u003e\n\u003cli\u003eReplace them with new symbol\u003c/li\u003e\n\u003cli\u003eRepeat\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThen, rare words will be split into multiple parts.\u003c/p\u003e\n\u003ch3\u003eCharacter-based NMT\u003c/h3\u003e\n\u003cp\u003eNo word segmentation. Istead of word embeddings - character group embedding. Challenge here is longer sequence length.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/rsennrich/subword-nmt\"\u003eSubword Neural Machine Translation\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003eMonolingual data for NMT\u003c/h3\u003e\n\u003cp\u003eAgain, available in a lot larger amounts. Even if it is better to have parallel data.\u003c/p\u003e\n\u003cp\u003eDecoder is similar to RNN language model.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTrain language model separately and combine with some weight in decoder.\u003c/li\u003e\n\u003cli\u003eSyntetic data. Get monolingual data of target language machine translated to source, even if not very correctly, and train on it like on ordinary parallel data. It will be trained on good target data, even with incorrect source, so it will train to generate correct text.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eMultilingual MT\u003c/h3\u003e\n\u003cp\u003eThere are about 6000 languages. Which gives about 36 millions possible translation directions. There are no parallel data for all this pairs. Parallel data exists mostly with English.\u003c/p\u003e\n\u003cp\u003eUse English as interlingua? But it is ambiguous.\u003c/p\u003e\n\u003cp\u003eUse language-independent representation with language-dependent encoder-decoder and shared attention mechanism.\u003c/p\u003e\n\u003cp\u003eThere is research from Facebook on \u003ca href=\"https://github.com/facebookresearch/LASER\"\u003eLanguage-Agnostic SEntence Representations\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003eNMT architectures\u003c/h3\u003e\n\u003cp\u003ePopular ones are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLSTM\u003c/li\u003e\n\u003cli\u003eTransformer architecture with self attention.\u003c/li\u003e\n\u003c/ul\u003e"}],"pages":1,"archives":[{"url":"/month/2021-01/page/1","id":"2021-01","count":4,"title":"2021-01"},{"url":"/month/2020-12/page/1","id":"2020-12","count":1,"title":"2020-12"},{"url":"/month/2020-09/page/1","id":"2020-09","count":2,"title":"2020-09"},{"url":"/month/2020-08/page/1","id":"2020-08","count":2,"title":"2020-08"},{"url":"/month/2020-07/page/1","id":"2020-07","count":10,"title":"2020-07"},{"url":"/month/2020-06/page/1","id":"2020-06","count":7,"title":"2020-06"},{"url":"/month/2018-12/page/1","id":"2018-12","count":1,"title":"2018-12"}],"topics":[{"url":"/tag/MOOC/page/1","id":"MOOC","title":"MOOC","count":2},{"url":"/tag/SICP/page/1","id":"SICP","title":"SICP","count":20},{"url":"/tag/ideas/page/1","id":"ideas","title":"ideas","count":1},{"url":"/tag/notes/page/1","id":"notes","title":"notes","count":5},{"url":"/tag/on writing/page/1","id":"on writing","title":"on writing","count":3}],"tag":"MOOC"},"__N_SSG":true},"page":"/tag/[tag]/page/[page]","query":{"tag":"MOOC","page":"1"},"buildId":"sMnLAto4YhgRliyLu0bmY","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>