<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>Machine Translation course notes</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/b53cc6ef9e9e352d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b53cc6ef9e9e352d.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-514908bffb652963.js" defer=""></script><script src="/_next/static/chunks/framework-91d7f78b5b4003c8.js" defer=""></script><script src="/_next/static/chunks/main-6099a486a931d74e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-9ca9bb1f507791d0.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-e383908e480c879a.js" defer=""></script><script src="/_next/static/BV4uJJEMRM41pRC-d7IhI/_buildManifest.js" defer=""></script><script src="/_next/static/BV4uJJEMRM41pRC-d7IhI/_ssgManifest.js" defer=""></script><script src="/_next/static/BV4uJJEMRM41pRC-d7IhI/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><h1 id="top">Taras Bunyk</h1><nav><a href="/">Home</a><a href="http://bunyk.github.io/mandala/">Snowflake drawing</a><a href="/resume">Resume</a><a href="https://bunyk.wordpress.com/">Old blog (in Ukrainian)</a></nav><article><h1>Machine Translation course notes</h1><div>Published: 2020-12-04</div><div><p>Recently finished Coursera course on <a href="https://www.coursera.org/learn/machinetranslation/">Machine Translation</a>. That was mostly overwiew course without any programming practice, but with a lot of links to scientific papers. So it is somewhat theoretical, but I would probably recommend if you want to dive into that field from research point of view.</p>
<p>So for example you get quizzed with questions like:
One important difference between phrase-based MT and n-gram based MT is:</p>
<ol>
<li>N-gram based MT has a fixed segmentation into units.</li>
<li>..</li>
</ol>
<p>But you will not be taught how to implement any of those. Instead you will get links to <a href="http://www.statmt.org/moses/">MOSES</a> &#x26; <a href="http://www.statmt.org/moses/giza/GIZA++.html">GIZA</a> projects. But probably course will help to understand that project.</p>
<p>I'm not at all a researcher, have no skill of reading the papers, and to get most of that course, tried to just write down notes. I have couple of promising areas into which I should dive deeper and experiment, to help me with my job of automating content creation for multilingual e-commerce site. And just on listening lectures, writing down notes and doing quizzes took me 14 hours measured in pomodoros to complete (already more than SICP, I should return back to it eventually). Then I'll probably try some hands-on experimentation <a href="https://www.tensorflow.org/tutorials/text/nmt_with_attention">with TensorFlow</a>.</p>
<p>So, here are typed notes made of my handwritten notes. They probably will not make much sense for you, especially because I'm to lazy to digitize hand-drawn diagrams, but maybe will help you to figure out what is inside the course and if you need to enroll into it yourself.</p>
<h2>History</h2>
<p>Translation has long history. Longest known parallel corpus of text is Bible, which is translated for thousands of years already.</p>
<p>People started to think about machine translation (MT for short), just when machines appeared - after WW2.</p>
<p>There was some "memorandum" of Warren Weaver in 1949, where he compared task of MT to task of decryption. And first model of translation was decryption model.</p>
<p>In 1954 there was a Georgetown experiment, where they translated 19 sentences from russian.</p>
<p>In 1966 was published ALPAC report, that said that MT failed to give meaningful results, and people mostly gave up on this.</p>
<p>1977 was the year of first successfull application of MT translation in limited scope - <a href="https://en.wikipedia.org/wiki/METEO_System">METEO system</a> for translating weahter forecasts from French to English. It used rule-based approach.</p>
<p>At the end of 1980s MT research resumed mostly with efforts by Japan.</p>
<p>First implementations of statistical machine translation appeared in 1990.</p>
<p>In 2005 MT have seen public adoption by organizations like Google, MicroSoft, EU Parlament...</p>
<h2>Approaches</h2>
<p>We could approach translation with different levels of abstraction (from highekst to lowest):</p>
<ul>
<li>Interlingua (encodes meaning, sematnic and pragmatic, but is hard to create, because creator needs to know all languages that will be used)</li>
<li>Semantic transfer</li>
<li>Syntactic transfer</li>
<li>Dictionary transfer/lookup</li>
</ul>
<p>This hierarchy is called Vaquois triangle.</p>
<p>Historically MT was approached with:</p>
<ol>
<li>Dictionary lookup (looks weird and is not correct)</li>
<li>Rule based MT (very laborious and manual)</li>
<li>Example based translation</li>
<li>Statistical translation (SMT)</li>
<li>Neural translation (NMT)</li>
</ol>
<p>3 to 5 are data driven, which means we don't need to reprogram system for new translations, just retrain it.</p>
<p>We could translate from and to different modalities - images, voice, etc. But different modalities bring in their challenges, like the need to infer punctuation from intonation, and need to train speach recognition, performance of which will influence performance of system overall.</p>
<h2>Data</h2>
<p>Success of MT is also influenced by availability of language data on the web. Modern MT systems train on gigawords datasets, while average human speaks 0.5 gigawords in a whole lifetime.</p>
<p>One common source of parallel data is European parlament, which publishes documents in 23 languages of EU.</p>
<h2>Performance</h2>
<p>Measuring performance of MT system is additional challenge. You could not just compare translations with unittests, as most of the sentences have multiple correct translations. Additionally apart from correctnes, we need to think about style transfer: formal/informal style, politeness, everyday/academic language etc.</p>
<p>There is popular scoring scheme for MT systems called BLEU.</p>
<h2>Use cases</h2>
<ul>
<li>assimilation: use MT like Google translate to understand foreign text.</li>
<li>dissemination: produce text in foreigh language. This use case needs a good quality. But here quality could be improved also by tuning source (reducing ambiguity, etc.)</li>
</ul>
<h2>Language</h2>
<p>Syntax of a language defines it's structure, or how to compose it's elements.</p>
<p>Semantics defines meaning.</p>
<p>Basic units of machine translation are usually words. And word segmentation is already a nontrivial problem. Winterschuhe in German is two words or one?</p>
<p>Morphology adds another challenge to the task of translation. Morphemes are built from stem &#x26; morphems.</p>
<p>Example Finnish -> English:</p>
<ul>
<li>talo -> house</li>
<li>talossa -> in house</li>
<li>talossani -> in my house</li>
<li>talossanikin -> in my house too</li>
<li>talossanikinko -> in my house too?</li>
</ul>
<p>Parts of speech define which role word plays in the sentence. Noun, adjective, verb, etc...</p>
<p>Grammatical category - property of items within grammar: Tense, Voice, Person, Gender, Number...</p>
<p>Agreement - when grammatical category of one word influences morphology. For example when noun and adjective should have same gender and number.</p>
<p>Sentence structure could be described by phrase structure grammar:</p>
<p>Jane buys a house</p>
<pre><code>S (sentence)
├── NP (noun phrase)
│   └── Noun
│       └── Jane
└── VP (verb phrase)
    ├── Verb
    │   └── Buys
    └── NP
        ├── article
        │   └── a
        └── Noun
            └── house
</code></pre>
<p>Semantics has important property of compositionality: meaning of a sentence is composed from meaning of its phrases.</p>
<p>Lexical semantics - meaning of single words.</p>
<p>It's difficult to derive semantics, because:</p>
<ul>
<li>Words could have multiple meanings
<ul>
<li>polysemy: one word could have multiple meanings: interest (in something, or %), bank (of a river, or financial institution).</li>
<li>homonymy: different words could have the same spelling: can (a verb, expresses ability to do action), can (a noun, for example: can of beans).</li>
</ul>
</li>
<li>Semantics is influenced by context</li>
<li>It is influenced by structure</li>
<li>Lexical mismatch. Word in one language could be described by different words in other languages, for example rice in japanese has one word for cooked rice, and another word for a rice plant. In spanish fish is "pez" when it is in the water, but "pezcado" when it is a dish.</li>
<li>Morphology also adds information. Lehrer, Lehrerin -> male &#x26; female teacher</li>
</ul>
<p>It is easier if source language has more information than target one.</p>
<ul>
<li>References
<ul>
<li>Co-reference (to something in sentenc or outside)</li>
<li>Anaphora: he, she, it</li>
<li>Deictic reference: hree, now, I.</li>
<li>Reference with synonym: house, buidling...</li>
</ul>
</li>
</ul>
<p>Example: "If the baby does not strive on the raw milk, boil it."</p>
<p>"It" referes to what?</p>
<ul>
<li>a) baby</li>
<li>b) milk</li>
</ul>
<p>Additional difficulty is that language is evolwing and constantly gains new words:</p>
<ul>
<li>Constructed: Brexit = Britain + Exit</li>
<li>Borrowed: downgeloaded (german for downloaded)</li>
</ul>
<p>Abbreviations, are another challenge for sematnics.</p>
<h2>Approaches</h2>
<p>Rule based MT: humans write rules, computer applies rules to text.</p>
<p>Corpora based: get big text corpora, apply machine learning.</p>
<p>Vauquois triangle:</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/f/f4/Direct_translation_and_transfer_translation_pyramid.svg" alt="Vauquois triangle"></p>
<h3>Rule-based</h3>
<p>Dictionary based. Does not work between languages where structure of sentence changes much.</p>
<p>Example of application of dictionary based approach - translation memory. Find similar already translated sentence and show to translator.</p>
<p>Transfer-based. 3-step approach:</p>
<ol>
<li>Analyze source and generate it's abstract representation (parse tree, morphology information etc.)</li>
<li>Transfer source representation to matching target representation using rules.</li>
<li>Generate target text from target representation. Ex: (house, plural) -> "houses".</li>
</ol>
<p>Interlingua - language for pure meaning. Just 2 steps:</p>
<ol>
<li>Analyze</li>
<li>Generate</li>
</ol>
<p>Interlingua approach helps in cases of having many language pairs.</p>
<p>But good interlingua is hard to find and it is done only for limited domains.</p>
<h3>Corpus-based</h3>
<p>No more manually written rules. We learn models from data.</p>
<p>It is usually easier to collect data then to have language experts that could write rules.</p>
<p>Statistical machine translation:</p>
<ol>
<li>Create alternative translations.</li>
<li>Score them.</li>
<li>Select one with best score.</li>
</ol>
<p>First SMT approaches were direct translations, advanced SMT uses interlingua representations.</p>
<p>Neural machine translation is also corpus based and is the latest approach developed for now. Has following advantages:</p>
<ul>
<li>Automatic learning of intermediate representation, no need to design it.</li>
<li>More compact representations than in SMT</li>
<li>Better translation quality</li>
</ul>
<h2>Data</h2>
<p>Is main knowledge source in corpus-based approach.</p>
<ul>
<li>Monolingual data (raw text) is available in huge amounts.</li>
<li>Parallel data: pairs of sentences in 2 languages.</li>
</ul>
<p>There are algorithms for sentence alignment for cases where we have parallel data aligned by documents, not by sentences.</p>
<p>Famous corpuses: TED corpus, EU parlament corpus.</p>
<p>Notation note: source sentences are marked with letter f (for French) and target with e (for English).</p>
<p>Data requires preprocessing to make text homohenous.</p>
<p>For example numbers are replaced with placeholder token, as they could refer to pages, which change from language to language, and we would not want to train on that.</p>
<p>Tokenization: mainly done by spaces and punctuation for european languages. Special cases are abbrefiations.</p>
<p>True-casing: some words should be uppercase, some lower-case, but at the beginning of the sentence they should be uppercase. Need to train on true case.</p>
<h2>Evaluation</h2>
<p>How to measure quality of translation? It is important, as you could only improve what you could measure.</p>
<p>Approaches: Human vs automatic.</p>
<p>Difficulties:</p>
<ul>
<li>Multiple translations are correct, we could not just compare strings.</li>
<li>Small changes could be very important. ("Let's eat grandma" vs "Let's eat, grandma")</li>
<li>Evaluation is subjective</li>
</ul>
<p>Human evaluation:</p>
<ul>
<li>Advantage:
<ul>
<li>Gold standard</li>
</ul>
</li>
<li>Disadvantages:
<ul>
<li>Subjective</li>
<li>Expensive</li>
<li>time consuming</li>
</ul>
</li>
</ul>
<p>Automatic</p>
<ul>
<li>Advantages:
<ul>
<li>Cheap</li>
<li>Fast</li>
</ul>
</li>
<li>Disadvantages:
<ul>
<li>still need human reference</li>
<li>difficult. If we made good evaluation program, maybe just use it to select best translation for us?</li>
</ul>
</li>
</ul>
<p>Granularity:</p>
<ul>
<li>Per sentence?</li>
<li>Per document?</li>
</ul>
<p>Task-based evaluation - is translation good for it's application?</p>
<p>Aspects of evaluation:</p>
<ul>
<li>Adequacy: is translation correct</li>
<li>Fluency: does it sound unusual in that language?</li>
</ul>
<p>Error analysis: where is the source of error? Wrong word, wrong order ... There is special software - BLAST - toolkit for error analysis.</p>
<h3>Human evaluation</h3>
<p>Expensive, so we need to minimize effort.</p>
<p>Stats:</p>
<ul>
<li><em>Inter annotator agreement</em> - how different annotators evaluate translations. Study was a 2016 shown that <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Cohen's kappa</a> coefficient (0 - random rankings, 1 - rankings are completely the same) for different annotators is 0.357</li>
<li><em>Intra annotagor agreement</em> - how same annotator ranks the same translation next time it is shown him: 0.529.</li>
</ul>
<p>So, translators tend to disagree about quality not even with others, but with themselves.</p>
<h4>Approaches</h4>
<p>Direct assesment:<br>
Given: source, translation.<br>
Evaluate: performance, adequacy, fluency at some scale.</p>
<p>Ranking:<br>
Given number of translations, select best of them, or rank them from best to worst.</p>
<p>Post editing:<br>
Measure time or keystrokes to edit machine translation into correct translation.</p>
<p>Task-based:<br>
evaluate translation performance where it will be used. Give students translated text, and then quiz them if they understood it.</p>
<ul>
<li>Advantage: this measures overall system performance.</li>
<li>Disadvantage:
<ul>
<li>complex</li>
<li>other factors, like quality of source, or some aspects of task influence result of evaluation</li>
</ul>
</li>
</ul>
<h2>BLEU: BiLingual Evaluation Understudy</h2>
<p>BLEU uses human translated examples from the same source as machine translated, and then checks how good they match.</p>
<p><span class="math math-inline"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">core</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.205em;"></span><span class="mord sqrt"><span class="root"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7002em;"><span style="top:-2.878em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size6 size1 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.835em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord text"><span class="mord">1-gram overlap</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord">2-gram overlap</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord">3-gram overlap</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord">4-gram overlap</span></span></span></span><span style="top:-2.795em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.205em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">BP</span></span></span></span></span></p>
<p>Brevity penalty: <span class="math math-inline"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">BP</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:6em;vertical-align:-2.75em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.25em;"><span style="top:-1.366em;"><span class="pstrut" style="height:3.216em;"></span><span class="delimsizinginner delim-size4"><span>⎩</span></span></span><span style="top:-1.358em;"><span class="pstrut" style="height:3.216em;"></span><span style="height:1.216em;width:0.8889em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.8889em" height="1.216em" style="width:0.8889em" viewBox="0 0 888.89 1216" preserveAspectRatio="xMinYMin"><path d="M384 0 H504 V1216 H384z M384 0 H504 V1216 H384z"></path></svg></span></span><span style="top:-3.216em;"><span class="pstrut" style="height:3.216em;"></span><span class="delimsizinginner delim-size4"><span>⎨</span></span></span><span style="top:-4.358em;"><span class="pstrut" style="height:3.216em;"></span><span style="height:1.216em;width:0.8889em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.8889em" height="1.216em" style="width:0.8889em" viewBox="0 0 888.89 1216" preserveAspectRatio="xMinYMin"><path d="M384 0 H504 V1216 H384z M384 0 H504 V1216 H384z"></path></svg></span></span><span style="top:-5.566em;"><span class="pstrut" style="height:3.216em;"></span><span class="delimsizinginner delim-size4"><span>⎧</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.75em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.2313em;"><span style="top:-5.434em;"><span class="pstrut" style="height:3.2107em;"></span><span class="mord"><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.13889em;">MT</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord">∣</span></span></span><span style="top:-3.994em;"><span class="pstrut" style="height:3.2107em;"></span><span class="mord"></span></span><span style="top:-2.554em;"><span class="pstrut" style="height:3.2107em;"></span><span class="mord"></span></span><span style="top:-0.9113em;"><span class="pstrut" style="height:3.2107em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.2107em;"><span style="top:-3.4842em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0378em;"><span style="top:-2.6408em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">MT</span><span class="mord mtight">∣</span></span></span></span><span style="top:-3.2255em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.5021em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mord mtight">∣</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5378em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.13889em;">MT</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord">∣</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.7313em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>BP is necessary to not rank too short translations (which loose information) too highly (as score is higher for shorter MT sentences).</p>
<h2>Statistical Machine translation</h2>
<p>Word based SMT. Do not directly translate, find probability of some sentence being a translation. As it is hard to compute probability of a sentence (it is small), we move down to words.</p>
<p>Lexicon: store possible translations for word. Example:</p>
<p>Wagen:</p>
<p>| Word    | Count | Probability |
|---------|-------|-------------|
| vehicle | 5000  | 0.5         |
| car     | 3000  | 0.3         |
| coach   | 1500  | 0.15        |
| wagon   | 500   | 0.05        |</p>
<p>Also we have alignment function from target words to source words (which translates to which). Target words additinallly have NULL to "translate" source words that does not exist in target.</p>
<p>Having lexicon and alignment function, we could implement <a href="https://en.wikipedia.org/wiki/IBM_alignment_models">IBM Model 1</a>. To get alignment function, we could use <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">expectation-maximization algorithm</a>.</p>
<h3>Language model</h3>
<p>Language model is a probability distribution of a sentences in language (how probable that given sentence is generated by native speaker of that language). It tries to model fluency (not accuracy).</p>
<p>Lots of sentences do not occur in training, but it's good to have P > 0 for them.</p>
<p><em>Markov assumption</em>: probability of word is approximated by n previous words before it.</p>
<p>Unknown n-gram => count = 0, P = 0 => P(sentence) = 0. This is bad, so we will need smoothing, where we shift some probability to unseen words.</p>
<p>One smoothing approach is to count unseen n-grams as occuring once, but that shifts a lot of probability to unknown.</p>
<p>Long n-grams are more precise, but a lot more sparse. We could use interpolation between long and short n-grams:</p>
<p>Most common smoothing model used today is modified Kneser-Ney smoothing.</p>
<h3>Translation</h3>
<p>Task of translation - to find the most probable translation e, given source sentence f.</p>
<p>Translation model: <span class="math math-inline"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">e</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mclose">)</span></span></span></span></span>
Language model: <span class="math math-inline"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">e</span><span class="mclose">)</span></span></span></span></span></p>
<p>How to combine two models?</p>
<p>Noisy channel approach from information theory. Assume source language was distorted into foreign language. Find most probable source message.</p>
<p>P(f) could be dropped, as it does not change depending on e.</p>
<p>Problem of this model is that often output is fluent, but not accurate. So we add weights to components to better tune it:</p>
<p>Such kind of interpolation is called log-linear model. With it we are not restricted to two features, but could have any number of them. And with any weight.</p>
<p>How to get optimal weights?</p>
<p>We could train in different stages:</p>
<ol>
<li>Learn language model and translation model.</li>
<li>Tuning: learn weights. Test different weights on validation data.</li>
</ol>
<p>Common way to speedup tuning, which could be very slow, because of many parameters &#x26; complex BLEU evaluation, is minimum error training:</p>
<ol>
<li>Start from random weights.</li>
<li>Apply Powell search. For all parameters:</li>
<li>Fix all parameters except current.</li>
<li>Find best value for current.</li>
<li>Save result, and start from 1 until having enough results.</li>
<li>Select best one.</li>
</ol>
<p>Need to evaluate translations using BLEU on whole evaluation set.</p>
<h3>Phrase based MT</h3>
<p>Instead of having words as a basic units - use phrases. Phrase is any sequence of words.</p>
<p>It shold be better, because there is no word-to-word correspondence between languages. For example:</p>
<ul>
<li>Idioms: kicked the bucket (en) = biss ins Grass (de)</li>
<li>Translation of word is context dependent: auf meine Kosten (de) = at my cost (en) => (auf = at), auf meinem Shiff = on my boat => (auf = on)</li>
</ul>
<p>We use IBM Models to generate Viterbi alignment.</p>
<p>After alignment - extract phrase pairs from sentences:</p>
<p>saw = gesehen haben
bisher = up till now</p>
<p>Includes words
was = what</p>
<p>And longer phrases:</p>
<p>what we seen up till now = was wir bisher gesehen haben</p>
<p>Now, estimate probability from corpus:</p>
<p>Up till now:</p>
<p>| f         | count | p(f|e) |
|-----------|-------|---------|
| bisher    | 20    | 20/70   |
| bis jetzt | 13    | 13/70   |
| bis heute | 8     | 8/70    |
| ...       | 29    | 29/70   |</p>
<p>But with log-linear model we could use multiple features to improve translation. For example, when we seen phrase only once:</p>
<p>Up till now</p>
<p>| f         | count | p(f|e) |
|-----------|-------|---------|
| bisher    | 1     | 1       |</p>
<p>Probability is 1, but we should not be so sure. So we could check inverse probability - how often we see "up till now" given "bisher".</p>
<p>Challenge of phrase-based MT is reordering. In German, for example verb could be split to be at the end and at the beginning of the sentence. So we could do some preordering first, so source sentence looks more like target one.</p>
<p>Or - use hierarchical phrases - phrases of phrases.</p>
<p>Was wir X gesehen haben -> What se saw X</p>
<p>Another approach - parts of speech language models. Build n-gram model from parts of speech: NOUN VERB NOUN. As there are a lot less parts of speech then words, n-grams could be a lot longer, without having a lot of sparsity. And having longer n-grams helps with ordering.</p>
<p>Also, cluster-based model could be used - automatically cluster words, when part of speech tagging is not available.</p>
<h2>Neural networks</h2>
<ul>
<li>Perceptron</li>
<li>Multi layer perceptron</li>
</ul>
<p>Size of hidden layers is a hyperparameter.</p>
<p>Error functions:</p>
<ul>
<li>Mean square error:</li>
</ul>
<ul>
<li>Cross entropy:</li>
</ul>
<p>o - output, t - desired output (target). x - each example from batch.</p>
<p>Stochastic gradient descent:</p>
<ol>
<li>Randomly take example.</li>
<li>Calculate error function.</li>
<li>Calculate gradient.</li>
<li>Update weihts with gradient.</li>
</ol>
<p>Model language using neural net.</p>
<p>Multilayer perceptron, where input - previous n-1 words, output - next word.</p>
<p>Problem - no fixed vocabulary. Look at most common words, replace the rest with UNKNOWN token.</p>
<p>Represent each word by index in frequency table.</p>
<p>Then use one-hot representation, because we don't want to have "the" be more close to "and" then to "a". In one-hot representation all the words are on the equal distance from each other.</p>
<p>Word embeding layer - group similar words into one embedding. Automatically learned and has less values then input. Used for each word separately and then output of word embedding for whole sentence is feeded into hidden layer.</p>
<p>Softmax activation function:</p>
<p>makes sure that sum of all outputs of layer is 1, and is good for modelling probability.</p>
<h3>Recurrent neural network language model</h3>
<p>Good for modelling long dependencies, where n-grams does not work good:</p>
<p>Ich melde mich ... an<br>
I register myself ...</p>
<p>register - melde an</p>
<p>Hidden state depends on input and previous hidden state output. Always insert one word and predict one next word. Use same one-hot representation with embeddings.</p>
<p>But it has wanishing gradient problem, backpropagation multiplies derivatives, and for first elements in sequence gradient is very small, so it learns very slowly. We try to fix this problem with special recurrent units: LSTM or GRU.</p>
<h3>Neural translation models</h3>
<p><strong>N-gram based NMT approach</strong>:</p>
<p>Reorder source to be similar to target. Extract translation units. Get pairs of minimal translation units.</p>
<p>Model probability of translating of "bisher" to "up till now", given history (previous translation units).</p>
<p>Challenge - there are a lot more possible translation units then words, as translation units are tuples of words.</p>
<p><strong>Joint models approach</strong>: Add source context to target language model.</p>
<p><strong>Discriminative word lexicon approach</strong>:
Predict target word based on all source words in source sentence. Bag of words representation, one hot is replaced with many-hot.</p>
<h3>Encoder-decoder model</h3>
<p>Sequence-to-sequence translation. No need for alignment. Whole system is trained together.</p>
<p>Source -> <strong>Encoder RNN</strong> -> hidden state of encoder is a sequence representation -> <strong>Decoder RNN</strong> -> Target</p>
<p>Advantage - it is simple. Disadvantage - has bottleneck - fixed size of sentence representation.</p>
<p>Note: TensorFlow has <a href="https://www.tensorflow.org/tutorials/text/nmt_with_attention">tutorial on sequence2sequence translation</a></p>
<p>Decoding: Beam search. It is like greedy search, where we each time select next most probable word, but here we find n most probable words, then predict from them, then again prune to n best results and coninue until the end.</p>
<h3>Attention based NMT</h3>
<p>Designed to overcome bottleneck of fixed representation between encoder &#x26; decoder.</p>
<p>Have sequence of hidden states.</p>
<p>Also, run RNN on reversed sentence, and as state depends most on the last input, we will have context with the next words. Compbine forward &#x26; backward representation and you will get representation for part of sentence.</p>
<p>How to determine which source states to use for given decoder state? Another neural network.</p>
<p>Conditional GRU. <em>Here it become very confusing to me.</em></p>
<h3>Training</h3>
<p>Same model differently randomly initialized will have different performances after training.</p>
<p><strong>Ensemble of models</strong> - average output of multiple models.</p>
<p>Advantage: better performance.
Disadvantages: Training speed, decoding speed.</p>
<p><strong>Weight averating</strong> - while training save model checkpoints. Error could increase when training for longer time, so it's better to have more models with different errors. Take model with average weights from different checkpoints (but not models from different trainings, will not work).</p>
<h3>Byte-pair encoding</h3>
<p>Another way to overcome vocabulary limitation (not fixed size, names, compound words, new words like brexit), except using UNKNOWN token, is to represent all possible words with just n symbols.</p>
<ol>
<li>Represent sequence with characters.</li>
<li>Find most frequent two characters.</li>
<li>Replace them with new symbol</li>
<li>Repeat</li>
</ol>
<p>Then, rare words will be split into multiple parts.</p>
<h3>Character-based NMT</h3>
<p>No word segmentation. Istead of word embeddings - character group embedding. Challenge here is longer sequence length.</p>
<p><a href="https://github.com/rsennrich/subword-nmt">Subword Neural Machine Translation</a></p>
<h3>Monolingual data for NMT</h3>
<p>Again, available in a lot larger amounts. Even if it is better to have parallel data.</p>
<p>Decoder is similar to RNN language model.</p>
<ul>
<li>Train language model separately and combine with some weight in decoder.</li>
<li>Syntetic data. Get monolingual data of target language machine translated to source, even if not very correctly, and train on it like on ordinary parallel data. It will be trained on good target data, even with incorrect source, so it will train to generate correct text.</li>
</ul>
<h3>Multilingual MT</h3>
<p>There are about 6000 languages. Which gives about 36 millions possible translation directions. There are no parallel data for all this pairs. Parallel data exists mostly with English.</p>
<p>Use English as interlingua? But it is ambiguous.</p>
<p>Use language-independent representation with language-dependent encoder-decoder and shared attention mechanism.</p>
<p>There is research from Facebook on <a href="https://github.com/facebookresearch/LASER">Language-Agnostic SEntence Representations</a>.</p>
<h3>NMT architectures</h3>
<p>Popular ones are:</p>
<ul>
<li>LSTM</li>
<li>Transformer architecture with self attention.</li>
</ul></div></article><nav><a href="/">Home</a><a href="#top">Back to top</a></nav><footer>© <!-- -->2021<!-- --> Bunyk Taras. Built with Next.js</footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"id":"coursera_mt","title":"Machine Translation course notes","date":"2020-12-04","toc":true,"mathjax":true,"tags":["MOOC","notes"],"content":"\u003cp\u003eRecently finished Coursera course on \u003ca href=\"https://www.coursera.org/learn/machinetranslation/\"\u003eMachine Translation\u003c/a\u003e. That was mostly overwiew course without any programming practice, but with a lot of links to scientific papers. So it is somewhat theoretical, but I would probably recommend if you want to dive into that field from research point of view.\u003c/p\u003e\n\u003cp\u003eSo for example you get quizzed with questions like:\nOne important difference between phrase-based MT and n-gram based MT is:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eN-gram based MT has a fixed segmentation into units.\u003c/li\u003e\n\u003cli\u003e..\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBut you will not be taught how to implement any of those. Instead you will get links to \u003ca href=\"http://www.statmt.org/moses/\"\u003eMOSES\u003c/a\u003e \u0026#x26; \u003ca href=\"http://www.statmt.org/moses/giza/GIZA++.html\"\u003eGIZA\u003c/a\u003e projects. But probably course will help to understand that project.\u003c/p\u003e\n\u003cp\u003eI'm not at all a researcher, have no skill of reading the papers, and to get most of that course, tried to just write down notes. I have couple of promising areas into which I should dive deeper and experiment, to help me with my job of automating content creation for multilingual e-commerce site. And just on listening lectures, writing down notes and doing quizzes took me 14 hours measured in pomodoros to complete (already more than SICP, I should return back to it eventually). Then I'll probably try some hands-on experimentation \u003ca href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\"\u003ewith TensorFlow\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSo, here are typed notes made of my handwritten notes. They probably will not make much sense for you, especially because I'm to lazy to digitize hand-drawn diagrams, but maybe will help you to figure out what is inside the course and if you need to enroll into it yourself.\u003c/p\u003e\n\u003ch2\u003eHistory\u003c/h2\u003e\n\u003cp\u003eTranslation has long history. Longest known parallel corpus of text is Bible, which is translated for thousands of years already.\u003c/p\u003e\n\u003cp\u003ePeople started to think about machine translation (MT for short), just when machines appeared - after WW2.\u003c/p\u003e\n\u003cp\u003eThere was some \"memorandum\" of Warren Weaver in 1949, where he compared task of MT to task of decryption. And first model of translation was decryption model.\u003c/p\u003e\n\u003cp\u003eIn 1954 there was a Georgetown experiment, where they translated 19 sentences from russian.\u003c/p\u003e\n\u003cp\u003eIn 1966 was published ALPAC report, that said that MT failed to give meaningful results, and people mostly gave up on this.\u003c/p\u003e\n\u003cp\u003e1977 was the year of first successfull application of MT translation in limited scope - \u003ca href=\"https://en.wikipedia.org/wiki/METEO_System\"\u003eMETEO system\u003c/a\u003e for translating weahter forecasts from French to English. It used rule-based approach.\u003c/p\u003e\n\u003cp\u003eAt the end of 1980s MT research resumed mostly with efforts by Japan.\u003c/p\u003e\n\u003cp\u003eFirst implementations of statistical machine translation appeared in 1990.\u003c/p\u003e\n\u003cp\u003eIn 2005 MT have seen public adoption by organizations like Google, MicroSoft, EU Parlament...\u003c/p\u003e\n\u003ch2\u003eApproaches\u003c/h2\u003e\n\u003cp\u003eWe could approach translation with different levels of abstraction (from highekst to lowest):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInterlingua (encodes meaning, sematnic and pragmatic, but is hard to create, because creator needs to know all languages that will be used)\u003c/li\u003e\n\u003cli\u003eSemantic transfer\u003c/li\u003e\n\u003cli\u003eSyntactic transfer\u003c/li\u003e\n\u003cli\u003eDictionary transfer/lookup\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis hierarchy is called Vaquois triangle.\u003c/p\u003e\n\u003cp\u003eHistorically MT was approached with:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDictionary lookup (looks weird and is not correct)\u003c/li\u003e\n\u003cli\u003eRule based MT (very laborious and manual)\u003c/li\u003e\n\u003cli\u003eExample based translation\u003c/li\u003e\n\u003cli\u003eStatistical translation (SMT)\u003c/li\u003e\n\u003cli\u003eNeural translation (NMT)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e3 to 5 are data driven, which means we don't need to reprogram system for new translations, just retrain it.\u003c/p\u003e\n\u003cp\u003eWe could translate from and to different modalities - images, voice, etc. But different modalities bring in their challenges, like the need to infer punctuation from intonation, and need to train speach recognition, performance of which will influence performance of system overall.\u003c/p\u003e\n\u003ch2\u003eData\u003c/h2\u003e\n\u003cp\u003eSuccess of MT is also influenced by availability of language data on the web. Modern MT systems train on gigawords datasets, while average human speaks 0.5 gigawords in a whole lifetime.\u003c/p\u003e\n\u003cp\u003eOne common source of parallel data is European parlament, which publishes documents in 23 languages of EU.\u003c/p\u003e\n\u003ch2\u003ePerformance\u003c/h2\u003e\n\u003cp\u003eMeasuring performance of MT system is additional challenge. You could not just compare translations with unittests, as most of the sentences have multiple correct translations. Additionally apart from correctnes, we need to think about style transfer: formal/informal style, politeness, everyday/academic language etc.\u003c/p\u003e\n\u003cp\u003eThere is popular scoring scheme for MT systems called BLEU.\u003c/p\u003e\n\u003ch2\u003eUse cases\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eassimilation: use MT like Google translate to understand foreign text.\u003c/li\u003e\n\u003cli\u003edissemination: produce text in foreigh language. This use case needs a good quality. But here quality could be improved also by tuning source (reducing ambiguity, etc.)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eLanguage\u003c/h2\u003e\n\u003cp\u003eSyntax of a language defines it's structure, or how to compose it's elements.\u003c/p\u003e\n\u003cp\u003eSemantics defines meaning.\u003c/p\u003e\n\u003cp\u003eBasic units of machine translation are usually words. And word segmentation is already a nontrivial problem. Winterschuhe in German is two words or one?\u003c/p\u003e\n\u003cp\u003eMorphology adds another challenge to the task of translation. Morphemes are built from stem \u0026#x26; morphems.\u003c/p\u003e\n\u003cp\u003eExample Finnish -\u003e English:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etalo -\u003e house\u003c/li\u003e\n\u003cli\u003etalossa -\u003e in house\u003c/li\u003e\n\u003cli\u003etalossani -\u003e in my house\u003c/li\u003e\n\u003cli\u003etalossanikin -\u003e in my house too\u003c/li\u003e\n\u003cli\u003etalossanikinko -\u003e in my house too?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eParts of speech define which role word plays in the sentence. Noun, adjective, verb, etc...\u003c/p\u003e\n\u003cp\u003eGrammatical category - property of items within grammar: Tense, Voice, Person, Gender, Number...\u003c/p\u003e\n\u003cp\u003eAgreement - when grammatical category of one word influences morphology. For example when noun and adjective should have same gender and number.\u003c/p\u003e\n\u003cp\u003eSentence structure could be described by phrase structure grammar:\u003c/p\u003e\n\u003cp\u003eJane buys a house\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eS (sentence)\n├── NP (noun phrase)\n│   └── Noun\n│       └── Jane\n└── VP (verb phrase)\n    ├── Verb\n    │   └── Buys\n    └── NP\n        ├── article\n        │   └── a\n        └── Noun\n            └── house\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSemantics has important property of compositionality: meaning of a sentence is composed from meaning of its phrases.\u003c/p\u003e\n\u003cp\u003eLexical semantics - meaning of single words.\u003c/p\u003e\n\u003cp\u003eIt's difficult to derive semantics, because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWords could have multiple meanings\n\u003cul\u003e\n\u003cli\u003epolysemy: one word could have multiple meanings: interest (in something, or %), bank (of a river, or financial institution).\u003c/li\u003e\n\u003cli\u003ehomonymy: different words could have the same spelling: can (a verb, expresses ability to do action), can (a noun, for example: can of beans).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSemantics is influenced by context\u003c/li\u003e\n\u003cli\u003eIt is influenced by structure\u003c/li\u003e\n\u003cli\u003eLexical mismatch. Word in one language could be described by different words in other languages, for example rice in japanese has one word for cooked rice, and another word for a rice plant. In spanish fish is \"pez\" when it is in the water, but \"pezcado\" when it is a dish.\u003c/li\u003e\n\u003cli\u003eMorphology also adds information. Lehrer, Lehrerin -\u003e male \u0026#x26; female teacher\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt is easier if source language has more information than target one.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eReferences\n\u003cul\u003e\n\u003cli\u003eCo-reference (to something in sentenc or outside)\u003c/li\u003e\n\u003cli\u003eAnaphora: he, she, it\u003c/li\u003e\n\u003cli\u003eDeictic reference: hree, now, I.\u003c/li\u003e\n\u003cli\u003eReference with synonym: house, buidling...\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExample: \"If the baby does not strive on the raw milk, boil it.\"\u003c/p\u003e\n\u003cp\u003e\"It\" referes to what?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea) baby\u003c/li\u003e\n\u003cli\u003eb) milk\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAdditional difficulty is that language is evolwing and constantly gains new words:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConstructed: Brexit = Britain + Exit\u003c/li\u003e\n\u003cli\u003eBorrowed: downgeloaded (german for downloaded)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAbbreviations, are another challenge for sematnics.\u003c/p\u003e\n\u003ch2\u003eApproaches\u003c/h2\u003e\n\u003cp\u003eRule based MT: humans write rules, computer applies rules to text.\u003c/p\u003e\n\u003cp\u003eCorpora based: get big text corpora, apply machine learning.\u003c/p\u003e\n\u003cp\u003eVauquois triangle:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://upload.wikimedia.org/wikipedia/commons/f/f4/Direct_translation_and_transfer_translation_pyramid.svg\" alt=\"Vauquois triangle\"\u003e\u003c/p\u003e\n\u003ch3\u003eRule-based\u003c/h3\u003e\n\u003cp\u003eDictionary based. Does not work between languages where structure of sentence changes much.\u003c/p\u003e\n\u003cp\u003eExample of application of dictionary based approach - translation memory. Find similar already translated sentence and show to translator.\u003c/p\u003e\n\u003cp\u003eTransfer-based. 3-step approach:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnalyze source and generate it's abstract representation (parse tree, morphology information etc.)\u003c/li\u003e\n\u003cli\u003eTransfer source representation to matching target representation using rules.\u003c/li\u003e\n\u003cli\u003eGenerate target text from target representation. Ex: (house, plural) -\u003e \"houses\".\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eInterlingua - language for pure meaning. Just 2 steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnalyze\u003c/li\u003e\n\u003cli\u003eGenerate\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eInterlingua approach helps in cases of having many language pairs.\u003c/p\u003e\n\u003cp\u003eBut good interlingua is hard to find and it is done only for limited domains.\u003c/p\u003e\n\u003ch3\u003eCorpus-based\u003c/h3\u003e\n\u003cp\u003eNo more manually written rules. We learn models from data.\u003c/p\u003e\n\u003cp\u003eIt is usually easier to collect data then to have language experts that could write rules.\u003c/p\u003e\n\u003cp\u003eStatistical machine translation:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCreate alternative translations.\u003c/li\u003e\n\u003cli\u003eScore them.\u003c/li\u003e\n\u003cli\u003eSelect one with best score.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eFirst SMT approaches were direct translations, advanced SMT uses interlingua representations.\u003c/p\u003e\n\u003cp\u003eNeural machine translation is also corpus based and is the latest approach developed for now. Has following advantages:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAutomatic learning of intermediate representation, no need to design it.\u003c/li\u003e\n\u003cli\u003eMore compact representations than in SMT\u003c/li\u003e\n\u003cli\u003eBetter translation quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eData\u003c/h2\u003e\n\u003cp\u003eIs main knowledge source in corpus-based approach.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMonolingual data (raw text) is available in huge amounts.\u003c/li\u003e\n\u003cli\u003eParallel data: pairs of sentences in 2 languages.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThere are algorithms for sentence alignment for cases where we have parallel data aligned by documents, not by sentences.\u003c/p\u003e\n\u003cp\u003eFamous corpuses: TED corpus, EU parlament corpus.\u003c/p\u003e\n\u003cp\u003eNotation note: source sentences are marked with letter f (for French) and target with e (for English).\u003c/p\u003e\n\u003cp\u003eData requires preprocessing to make text homohenous.\u003c/p\u003e\n\u003cp\u003eFor example numbers are replaced with placeholder token, as they could refer to pages, which change from language to language, and we would not want to train on that.\u003c/p\u003e\n\u003cp\u003eTokenization: mainly done by spaces and punctuation for european languages. Special cases are abbrefiations.\u003c/p\u003e\n\u003cp\u003eTrue-casing: some words should be uppercase, some lower-case, but at the beginning of the sentence they should be uppercase. Need to train on true case.\u003c/p\u003e\n\u003ch2\u003eEvaluation\u003c/h2\u003e\n\u003cp\u003eHow to measure quality of translation? It is important, as you could only improve what you could measure.\u003c/p\u003e\n\u003cp\u003eApproaches: Human vs automatic.\u003c/p\u003e\n\u003cp\u003eDifficulties:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMultiple translations are correct, we could not just compare strings.\u003c/li\u003e\n\u003cli\u003eSmall changes could be very important. (\"Let's eat grandma\" vs \"Let's eat, grandma\")\u003c/li\u003e\n\u003cli\u003eEvaluation is subjective\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHuman evaluation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvantage:\n\u003cul\u003e\n\u003cli\u003eGold standard\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDisadvantages:\n\u003cul\u003e\n\u003cli\u003eSubjective\u003c/li\u003e\n\u003cli\u003eExpensive\u003c/li\u003e\n\u003cli\u003etime consuming\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAutomatic\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvantages:\n\u003cul\u003e\n\u003cli\u003eCheap\u003c/li\u003e\n\u003cli\u003eFast\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDisadvantages:\n\u003cul\u003e\n\u003cli\u003estill need human reference\u003c/li\u003e\n\u003cli\u003edifficult. If we made good evaluation program, maybe just use it to select best translation for us?\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eGranularity:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePer sentence?\u003c/li\u003e\n\u003cli\u003ePer document?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTask-based evaluation - is translation good for it's application?\u003c/p\u003e\n\u003cp\u003eAspects of evaluation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdequacy: is translation correct\u003c/li\u003e\n\u003cli\u003eFluency: does it sound unusual in that language?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eError analysis: where is the source of error? Wrong word, wrong order ... There is special software - BLAST - toolkit for error analysis.\u003c/p\u003e\n\u003ch3\u003eHuman evaluation\u003c/h3\u003e\n\u003cp\u003eExpensive, so we need to minimize effort.\u003c/p\u003e\n\u003cp\u003eStats:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eInter annotator agreement\u003c/em\u003e - how different annotators evaluate translations. Study was a 2016 shown that \u003ca href=\"https://en.wikipedia.org/wiki/Cohen%27s_kappa\"\u003eCohen's kappa\u003c/a\u003e coefficient (0 - random rankings, 1 - rankings are completely the same) for different annotators is 0.357\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eIntra annotagor agreement\u003c/em\u003e - how same annotator ranks the same translation next time it is shown him: 0.529.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSo, translators tend to disagree about quality not even with others, but with themselves.\u003c/p\u003e\n\u003ch4\u003eApproaches\u003c/h4\u003e\n\u003cp\u003eDirect assesment:\u003cbr\u003e\nGiven: source, translation.\u003cbr\u003e\nEvaluate: performance, adequacy, fluency at some scale.\u003c/p\u003e\n\u003cp\u003eRanking:\u003cbr\u003e\nGiven number of translations, select best of them, or rank them from best to worst.\u003c/p\u003e\n\u003cp\u003ePost editing:\u003cbr\u003e\nMeasure time or keystrokes to edit machine translation into correct translation.\u003c/p\u003e\n\u003cp\u003eTask-based:\u003cbr\u003e\nevaluate translation performance where it will be used. Give students translated text, and then quiz them if they understood it.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvantage: this measures overall system performance.\u003c/li\u003e\n\u003cli\u003eDisadvantage:\n\u003cul\u003e\n\u003cli\u003ecomplex\u003c/li\u003e\n\u003cli\u003eother factors, like quality of source, or some aspects of task influence result of evaluation\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eBLEU: BiLingual Evaluation Understudy\u003c/h2\u003e\n\u003cp\u003eBLEU uses human translated examples from the same source as machine translated, and then checks how good they match.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.05764em;\"\u003eS\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ecore\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.04em;vertical-align:-0.205em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord sqrt\"\u003e\u003cspan class=\"root\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.7002em;\"\u003e\u003cspan style=\"top:-2.878em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.5em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size1 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e4\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.835em;\"\u003e\u003cspan class=\"svg-align\" style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\" style=\"padding-left:0.833em;\"\u003e\u003cspan class=\"mord text\"\u003e\u003cspan class=\"mord\"\u003e1-gram overlap\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord text\"\u003e\u003cspan class=\"mord\"\u003e2-gram overlap\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord text\"\u003e\u003cspan class=\"mord\"\u003e3-gram overlap\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord text\"\u003e\u003cspan class=\"mord\"\u003e4-gram overlap\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-2.795em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"400em\" height=\"1.08em\" viewBox=\"0 0 400000 1080\" preserveAspectRatio=\"xMinYMin slice\"\u003e\u003cpath d=\"M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.205em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eBP\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eBrevity penalty: \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eBP\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:6em;vertical-align:-2.75em;\"\u003e\u003c/span\u003e\u003cspan class=\"minner\"\u003e\u003cspan class=\"mopen\"\u003e\u003cspan class=\"delimsizing mult\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:3.25em;\"\u003e\u003cspan style=\"top:-1.366em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.216em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎩\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-1.358em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.216em;\"\u003e\u003c/span\u003e\u003cspan style=\"height:1.216em;width:0.8889em;\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"0.8889em\" height=\"1.216em\" style=\"width:0.8889em\" viewBox=\"0 0 888.89 1216\" preserveAspectRatio=\"xMinYMin\"\u003e\u003cpath d=\"M384 0 H504 V1216 H384z M384 0 H504 V1216 H384z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.216em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.216em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎨\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-4.358em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.216em;\"\u003e\u003c/span\u003e\u003cspan style=\"height:1.216em;width:0.8889em;\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"0.8889em\" height=\"1.216em\" style=\"width:0.8889em\" viewBox=\"0 0 888.89 1216\" preserveAspectRatio=\"xMinYMin\"\u003e\u003cpath d=\"M384 0 H504 V1216 H384z M384 0 H504 V1216 H384z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-5.566em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.216em;\"\u003e\u003c/span\u003e\u003cspan class=\"delimsizinginner delim-size4\"\u003e\u003cspan\u003e⎧\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:2.75em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mtable\"\u003e\u003cspan class=\"col-align-l\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:3.2313em;\"\u003e\u003cspan style=\"top:-5.434em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.2107em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eMT\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.08125em;\"\u003eH\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.994em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.2107em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-2.554em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.2107em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-0.9113em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3.2107em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:1.2107em;\"\u003e\u003cspan style=\"top:-3.4842em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mopen nulldelimiter sizing reset-size3 size6\"\u003e\u003c/span\u003e\u003cspan class=\"mfrac\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:1.0378em;\"\u003e\u003cspan style=\"top:-2.6408em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size3 size1 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e∣\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003eMT\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e∣\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.2255em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.5021em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size3 size1 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e∣\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.08125em;\"\u003eH\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e∣\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.5378em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose nulldelimiter sizing reset-size3 size6\"\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eMT\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e≤\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.08125em;\"\u003eH\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:2.7313em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose nulldelimiter\"\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eBP is necessary to not rank too short translations (which loose information) too highly (as score is higher for shorter MT sentences).\u003c/p\u003e\n\u003ch2\u003eStatistical Machine translation\u003c/h2\u003e\n\u003cp\u003eWord based SMT. Do not directly translate, find probability of some sentence being a translation. As it is hard to compute probability of a sentence (it is small), we move down to words.\u003c/p\u003e\n\u003cp\u003eLexicon: store possible translations for word. Example:\u003c/p\u003e\n\u003cp\u003eWagen:\u003c/p\u003e\n\u003cp\u003e| Word    | Count | Probability |\n|---------|-------|-------------|\n| vehicle | 5000  | 0.5         |\n| car     | 3000  | 0.3         |\n| coach   | 1500  | 0.15        |\n| wagon   | 500   | 0.05        |\u003c/p\u003e\n\u003cp\u003eAlso we have alignment function from target words to source words (which translates to which). Target words additinallly have NULL to \"translate\" source words that does not exist in target.\u003c/p\u003e\n\u003cp\u003eHaving lexicon and alignment function, we could implement \u003ca href=\"https://en.wikipedia.org/wiki/IBM_alignment_models\"\u003eIBM Model 1\u003c/a\u003e. To get alignment function, we could use \u003ca href=\"https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\"\u003eexpectation-maximization algorithm\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003eLanguage model\u003c/h3\u003e\n\u003cp\u003eLanguage model is a probability distribution of a sentences in language (how probable that given sentence is generated by native speaker of that language). It tries to model fluency (not accuracy).\u003c/p\u003e\n\u003cp\u003eLots of sentences do not occur in training, but it's good to have P \u003e 0 for them.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMarkov assumption\u003c/em\u003e: probability of word is approximated by n previous words before it.\u003c/p\u003e\n\u003cp\u003eUnknown n-gram =\u003e count = 0, P = 0 =\u003e P(sentence) = 0. This is bad, so we will need smoothing, where we shift some probability to unseen words.\u003c/p\u003e\n\u003cp\u003eOne smoothing approach is to count unseen n-grams as occuring once, but that shifts a lot of probability to unknown.\u003c/p\u003e\n\u003cp\u003eLong n-grams are more precise, but a lot more sparse. We could use interpolation between long and short n-grams:\u003c/p\u003e\n\u003cp\u003eMost common smoothing model used today is modified Kneser-Ney smoothing.\u003c/p\u003e\n\u003ch3\u003eTranslation\u003c/h3\u003e\n\u003cp\u003eTask of translation - to find the most probable translation e, given source sentence f.\u003c/p\u003e\n\u003cp\u003eTranslation model: \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eP\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003ef\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nLanguage model: \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eP\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eHow to combine two models?\u003c/p\u003e\n\u003cp\u003eNoisy channel approach from information theory. Assume source language was distorted into foreign language. Find most probable source message.\u003c/p\u003e\n\u003cp\u003eP(f) could be dropped, as it does not change depending on e.\u003c/p\u003e\n\u003cp\u003eProblem of this model is that often output is fluent, but not accurate. So we add weights to components to better tune it:\u003c/p\u003e\n\u003cp\u003eSuch kind of interpolation is called log-linear model. With it we are not restricted to two features, but could have any number of them. And with any weight.\u003c/p\u003e\n\u003cp\u003eHow to get optimal weights?\u003c/p\u003e\n\u003cp\u003eWe could train in different stages:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLearn language model and translation model.\u003c/li\u003e\n\u003cli\u003eTuning: learn weights. Test different weights on validation data.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCommon way to speedup tuning, which could be very slow, because of many parameters \u0026#x26; complex BLEU evaluation, is minimum error training:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eStart from random weights.\u003c/li\u003e\n\u003cli\u003eApply Powell search. For all parameters:\u003c/li\u003e\n\u003cli\u003eFix all parameters except current.\u003c/li\u003e\n\u003cli\u003eFind best value for current.\u003c/li\u003e\n\u003cli\u003eSave result, and start from 1 until having enough results.\u003c/li\u003e\n\u003cli\u003eSelect best one.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNeed to evaluate translations using BLEU on whole evaluation set.\u003c/p\u003e\n\u003ch3\u003ePhrase based MT\u003c/h3\u003e\n\u003cp\u003eInstead of having words as a basic units - use phrases. Phrase is any sequence of words.\u003c/p\u003e\n\u003cp\u003eIt shold be better, because there is no word-to-word correspondence between languages. For example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIdioms: kicked the bucket (en) = biss ins Grass (de)\u003c/li\u003e\n\u003cli\u003eTranslation of word is context dependent: auf meine Kosten (de) = at my cost (en) =\u003e (auf = at), auf meinem Shiff = on my boat =\u003e (auf = on)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe use IBM Models to generate Viterbi alignment.\u003c/p\u003e\n\u003cp\u003eAfter alignment - extract phrase pairs from sentences:\u003c/p\u003e\n\u003cp\u003esaw = gesehen haben\nbisher = up till now\u003c/p\u003e\n\u003cp\u003eIncludes words\nwas = what\u003c/p\u003e\n\u003cp\u003eAnd longer phrases:\u003c/p\u003e\n\u003cp\u003ewhat we seen up till now = was wir bisher gesehen haben\u003c/p\u003e\n\u003cp\u003eNow, estimate probability from corpus:\u003c/p\u003e\n\u003cp\u003eUp till now:\u003c/p\u003e\n\u003cp\u003e| f         | count | p(f|e) |\n|-----------|-------|---------|\n| bisher    | 20    | 20/70   |\n| bis jetzt | 13    | 13/70   |\n| bis heute | 8     | 8/70    |\n| ...       | 29    | 29/70   |\u003c/p\u003e\n\u003cp\u003eBut with log-linear model we could use multiple features to improve translation. For example, when we seen phrase only once:\u003c/p\u003e\n\u003cp\u003eUp till now\u003c/p\u003e\n\u003cp\u003e| f         | count | p(f|e) |\n|-----------|-------|---------|\n| bisher    | 1     | 1       |\u003c/p\u003e\n\u003cp\u003eProbability is 1, but we should not be so sure. So we could check inverse probability - how often we see \"up till now\" given \"bisher\".\u003c/p\u003e\n\u003cp\u003eChallenge of phrase-based MT is reordering. In German, for example verb could be split to be at the end and at the beginning of the sentence. So we could do some preordering first, so source sentence looks more like target one.\u003c/p\u003e\n\u003cp\u003eOr - use hierarchical phrases - phrases of phrases.\u003c/p\u003e\n\u003cp\u003eWas wir X gesehen haben -\u003e What se saw X\u003c/p\u003e\n\u003cp\u003eAnother approach - parts of speech language models. Build n-gram model from parts of speech: NOUN VERB NOUN. As there are a lot less parts of speech then words, n-grams could be a lot longer, without having a lot of sparsity. And having longer n-grams helps with ordering.\u003c/p\u003e\n\u003cp\u003eAlso, cluster-based model could be used - automatically cluster words, when part of speech tagging is not available.\u003c/p\u003e\n\u003ch2\u003eNeural networks\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePerceptron\u003c/li\u003e\n\u003cli\u003eMulti layer perceptron\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSize of hidden layers is a hyperparameter.\u003c/p\u003e\n\u003cp\u003eError functions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMean square error:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eCross entropy:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eo - output, t - desired output (target). x - each example from batch.\u003c/p\u003e\n\u003cp\u003eStochastic gradient descent:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRandomly take example.\u003c/li\u003e\n\u003cli\u003eCalculate error function.\u003c/li\u003e\n\u003cli\u003eCalculate gradient.\u003c/li\u003e\n\u003cli\u003eUpdate weihts with gradient.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eModel language using neural net.\u003c/p\u003e\n\u003cp\u003eMultilayer perceptron, where input - previous n-1 words, output - next word.\u003c/p\u003e\n\u003cp\u003eProblem - no fixed vocabulary. Look at most common words, replace the rest with UNKNOWN token.\u003c/p\u003e\n\u003cp\u003eRepresent each word by index in frequency table.\u003c/p\u003e\n\u003cp\u003eThen use one-hot representation, because we don't want to have \"the\" be more close to \"and\" then to \"a\". In one-hot representation all the words are on the equal distance from each other.\u003c/p\u003e\n\u003cp\u003eWord embeding layer - group similar words into one embedding. Automatically learned and has less values then input. Used for each word separately and then output of word embedding for whole sentence is feeded into hidden layer.\u003c/p\u003e\n\u003cp\u003eSoftmax activation function:\u003c/p\u003e\n\u003cp\u003emakes sure that sum of all outputs of layer is 1, and is good for modelling probability.\u003c/p\u003e\n\u003ch3\u003eRecurrent neural network language model\u003c/h3\u003e\n\u003cp\u003eGood for modelling long dependencies, where n-grams does not work good:\u003c/p\u003e\n\u003cp\u003eIch melde mich ... an\u003cbr\u003e\nI register myself ...\u003c/p\u003e\n\u003cp\u003eregister - melde an\u003c/p\u003e\n\u003cp\u003eHidden state depends on input and previous hidden state output. Always insert one word and predict one next word. Use same one-hot representation with embeddings.\u003c/p\u003e\n\u003cp\u003eBut it has wanishing gradient problem, backpropagation multiplies derivatives, and for first elements in sequence gradient is very small, so it learns very slowly. We try to fix this problem with special recurrent units: LSTM or GRU.\u003c/p\u003e\n\u003ch3\u003eNeural translation models\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eN-gram based NMT approach\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eReorder source to be similar to target. Extract translation units. Get pairs of minimal translation units.\u003c/p\u003e\n\u003cp\u003eModel probability of translating of \"bisher\" to \"up till now\", given history (previous translation units).\u003c/p\u003e\n\u003cp\u003eChallenge - there are a lot more possible translation units then words, as translation units are tuples of words.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eJoint models approach\u003c/strong\u003e: Add source context to target language model.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDiscriminative word lexicon approach\u003c/strong\u003e:\nPredict target word based on all source words in source sentence. Bag of words representation, one hot is replaced with many-hot.\u003c/p\u003e\n\u003ch3\u003eEncoder-decoder model\u003c/h3\u003e\n\u003cp\u003eSequence-to-sequence translation. No need for alignment. Whole system is trained together.\u003c/p\u003e\n\u003cp\u003eSource -\u003e \u003cstrong\u003eEncoder RNN\u003c/strong\u003e -\u003e hidden state of encoder is a sequence representation -\u003e \u003cstrong\u003eDecoder RNN\u003c/strong\u003e -\u003e Target\u003c/p\u003e\n\u003cp\u003eAdvantage - it is simple. Disadvantage - has bottleneck - fixed size of sentence representation.\u003c/p\u003e\n\u003cp\u003eNote: TensorFlow has \u003ca href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\"\u003etutorial on sequence2sequence translation\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDecoding: Beam search. It is like greedy search, where we each time select next most probable word, but here we find n most probable words, then predict from them, then again prune to n best results and coninue until the end.\u003c/p\u003e\n\u003ch3\u003eAttention based NMT\u003c/h3\u003e\n\u003cp\u003eDesigned to overcome bottleneck of fixed representation between encoder \u0026#x26; decoder.\u003c/p\u003e\n\u003cp\u003eHave sequence of hidden states.\u003c/p\u003e\n\u003cp\u003eAlso, run RNN on reversed sentence, and as state depends most on the last input, we will have context with the next words. Compbine forward \u0026#x26; backward representation and you will get representation for part of sentence.\u003c/p\u003e\n\u003cp\u003eHow to determine which source states to use for given decoder state? Another neural network.\u003c/p\u003e\n\u003cp\u003eConditional GRU. \u003cem\u003eHere it become very confusing to me.\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003eTraining\u003c/h3\u003e\n\u003cp\u003eSame model differently randomly initialized will have different performances after training.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEnsemble of models\u003c/strong\u003e - average output of multiple models.\u003c/p\u003e\n\u003cp\u003eAdvantage: better performance.\nDisadvantages: Training speed, decoding speed.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWeight averating\u003c/strong\u003e - while training save model checkpoints. Error could increase when training for longer time, so it's better to have more models with different errors. Take model with average weights from different checkpoints (but not models from different trainings, will not work).\u003c/p\u003e\n\u003ch3\u003eByte-pair encoding\u003c/h3\u003e\n\u003cp\u003eAnother way to overcome vocabulary limitation (not fixed size, names, compound words, new words like brexit), except using UNKNOWN token, is to represent all possible words with just n symbols.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRepresent sequence with characters.\u003c/li\u003e\n\u003cli\u003eFind most frequent two characters.\u003c/li\u003e\n\u003cli\u003eReplace them with new symbol\u003c/li\u003e\n\u003cli\u003eRepeat\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThen, rare words will be split into multiple parts.\u003c/p\u003e\n\u003ch3\u003eCharacter-based NMT\u003c/h3\u003e\n\u003cp\u003eNo word segmentation. Istead of word embeddings - character group embedding. Challenge here is longer sequence length.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/rsennrich/subword-nmt\"\u003eSubword Neural Machine Translation\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003eMonolingual data for NMT\u003c/h3\u003e\n\u003cp\u003eAgain, available in a lot larger amounts. Even if it is better to have parallel data.\u003c/p\u003e\n\u003cp\u003eDecoder is similar to RNN language model.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTrain language model separately and combine with some weight in decoder.\u003c/li\u003e\n\u003cli\u003eSyntetic data. Get monolingual data of target language machine translated to source, even if not very correctly, and train on it like on ordinary parallel data. It will be trained on good target data, even with incorrect source, so it will train to generate correct text.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eMultilingual MT\u003c/h3\u003e\n\u003cp\u003eThere are about 6000 languages. Which gives about 36 millions possible translation directions. There are no parallel data for all this pairs. Parallel data exists mostly with English.\u003c/p\u003e\n\u003cp\u003eUse English as interlingua? But it is ambiguous.\u003c/p\u003e\n\u003cp\u003eUse language-independent representation with language-dependent encoder-decoder and shared attention mechanism.\u003c/p\u003e\n\u003cp\u003eThere is research from Facebook on \u003ca href=\"https://github.com/facebookresearch/LASER\"\u003eLanguage-Agnostic SEntence Representations\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003eNMT architectures\u003c/h3\u003e\n\u003cp\u003ePopular ones are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLSTM\u003c/li\u003e\n\u003cli\u003eTransformer architecture with self attention.\u003c/li\u003e\n\u003c/ul\u003e"},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"coursera_mt"},"buildId":"BV4uJJEMRM41pRC-d7IhI","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>